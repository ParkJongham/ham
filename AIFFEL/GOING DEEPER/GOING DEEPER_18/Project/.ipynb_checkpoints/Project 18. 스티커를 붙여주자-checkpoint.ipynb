{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 프로젝트 : 스티커를 붙여주자\n",
    "\n",
    "드디어 딥러닝 모델을 이용해 우리의 스티커앱을 개선해서, 이미지 속 다수의 사람에게 스티커를 붙여줄 수 있게 되었습니다. 위 그림의 예시처럼 말이죠.\n",
    "\n",
    "여러분들도 여러분들의 작품을 통해 이 작업을 본격적으로 수행해 봅시다. 첫번째 스텝에서 제공해 드린 프로젝트 코드를 적극 활용하셔도 좋습니다.\n",
    "\n",
    "\n",
    "## Step 1. 스티커 구하기 혹은 만들기\n",
    "\n",
    "여러분들은 이미 [카메라 스티커앱 만들기 첫걸음] 노드를 수행해 보셨을 것입니다. 왕관 또는 고양이 수염, 혹은 다양한 아이디어의 스티커를 만들어 볼 수 있을 것입니다. 당시의 스티커 이미지를 그대로 활용하셔도 되고, 다른 이미지를 사용하셔도 무방합니다.\n",
    "\n",
    "\n",
    "## Step 2. SSD 모델을 통해 얼굴 bounding box 찾기\n",
    "\n",
    "우리는 실습코드를 진행하며 필요한 모델을 이미 생성해 왔을 것입니다. 잘 훈련된 해당 모델을 통해 적절한 얼굴 bounding box를 찾아내 봅시다.  `inference.py`  코드를 적극 참고해 보시기를 권합니다.\n",
    "\n",
    "\n",
    "## Step 3. dlib 을 이용한 landmark 찾기\n",
    "\n",
    "검출된 bounding box별로 dlib을 이용해 face landmark를 찾을 수 있을 것입니다.  `inference.py`  에서  `show_image`  메소드를 사용한 부분을 잘 수정하면 가능할 것입니다.\n",
    "\n",
    "\n",
    "## Step 4. 스티커 합성 사진 생성하기\n",
    "\n",
    "여러분들이 선택한 인물사진에 스티커를 합성해 봅시다. 이미지에 너무 많은 사람 얼굴이 포함되어 있거나, 검출된 얼굴이 너무 작아서 스티커 합성이 어울리지 않으면 적당하지 않겠죠? 3~5명 정도의 얼굴이 포함된 적당한 사진을 선택해 주세요.\n",
    "\n",
    "1명의 얼굴에 스티커를 붙여주는 방법은 이미 알고 계실 것입니다. 생성된 이미지를 프로젝트 코드와 함께 제출해 주세요~\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 스티커 구하기 혹은 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SSD 모델을 통해 얼굴 bounding box 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 전처리\n",
    "\n",
    "# bounding box 관련 주요 좌표 정의\n",
    "\n",
    "def get_box(data):\n",
    "    x0 = int(data[0])\n",
    "    y0 = int(data[1])\n",
    "    w = int(data[2])\n",
    "    h = int(data[3])\n",
    "    return x0, y0, w, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding box 관련 파일분석하는 코드 \n",
    "\n",
    "def parse_widerface(config_path):\n",
    "    boxes_per_img = []\n",
    "    with open(config_path) as fp:\n",
    "        line = fp.readline()\n",
    "        cnt = 1\n",
    "        while line:\n",
    "            num_of_obj = int(fp.readline())\n",
    "            boxes = []\n",
    "            for i in range(num_of_obj):\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                if w == 0:\n",
    "                    # remove boxes with no width\n",
    "                    continue\n",
    "                if h == 0:\n",
    "                    # remove boxes with no height\n",
    "                    continue\n",
    "                # Because our network is outputting 7x7 grid then it's not worth processing images with more than\n",
    "                # 5 faces because it's highly probable they are close to each other.\n",
    "                # You could remove this filter if you decide to switch to larger grid (like 14x14)\n",
    "                # Don't worry about number of train data because even with this filter we have around 16k samples\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            if num_of_obj == 0:\n",
    "                obj_box = fp.readline().split(' ')\n",
    "                x0, y0, w, h = get_box(obj_box)\n",
    "                boxes.append([x0, y0, w, h])\n",
    "            boxes_per_img.append((line.strip(), boxes))\n",
    "            line = fp.readline()\n",
    "            cnt += 1\n",
    "\n",
    "    return boxes_per_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding box 정보를 파싱해 리스트로 추출\n",
    "\n",
    "def process_image(image_file):\n",
    "    image_string = tf.io.read_file(image_file)\n",
    "    try:\n",
    "        image_data = tf.image.decode_jpeg(image_string, channels=3)\n",
    "        return 0, image_string, image_data\n",
    "    except tf.errors.InvalidArgumentError:\n",
    "        logging.info('{}: Invalid JPEG data or crop window'.format(image_file))\n",
    "        return 1, image_string, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh_to_voc(file_name, boxes, image_data):\n",
    "    shape = image_data.shape\n",
    "    image_info = {}\n",
    "    image_info['filename'] = file_name\n",
    "    image_info['width'] = shape[1]\n",
    "    image_info['height'] = shape[0]\n",
    "    image_info['depth'] = 3\n",
    "\n",
    "    difficult = []\n",
    "    classes = []\n",
    "    xmin, ymin, xmax, ymax = [], [], [], []\n",
    "\n",
    "    for box in boxes:\n",
    "        classes.append(1)\n",
    "        difficult.append(0)\n",
    "        xmin.append(box[0])\n",
    "        ymin.append(box[1])\n",
    "        xmax.append(box[0] + box[2])\n",
    "        ymax.append(box[1] + box[3])\n",
    "    image_info['class'] = classes\n",
    "    image_info['xmin'] = xmin\n",
    "    image_info['ymin'] = ymin\n",
    "    image_info['xmax'] = xmax\n",
    "    image_info['ymax'] = ymax\n",
    "    image_info['difficult'] = difficult\n",
    "\n",
    "    return image_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "{'filename': '/home/aiffel/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_849.jpg', 'width': 1024, 'height': 1385, 'depth': 3, 'class': [1], 'xmin': [449], 'ymin': [330], 'xmax': [571], 'ymax': [479], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_Parade_0_904.jpg', 'width': 1024, 'height': 1432, 'depth': 3, 'class': [1], 'xmin': [361], 'ymin': [98], 'xmax': [624], 'ymax': [437], 'difficult': [0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_799.jpg', 'width': 1024, 'height': 768, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [78, 78, 113, 134, 163, 201, 182, 245, 304, 328, 389, 406, 436, 522, 643, 653, 793, 535, 29, 3, 20], 'ymin': [221, 238, 212, 260, 250, 218, 266, 279, 265, 295, 281, 293, 290, 328, 320, 224, 337, 311, 220, 232, 215], 'xmax': [85, 92, 124, 149, 177, 211, 197, 263, 320, 344, 406, 427, 458, 543, 666, 670, 816, 551, 40, 14, 32], 'ymax': [229, 255, 227, 275, 267, 230, 283, 294, 282, 315, 300, 314, 307, 346, 342, 249, 367, 328, 235, 247, 231], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_117.jpg', 'width': 1024, 'height': 682, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [69, 227, 296, 353, 885, 819, 727, 598, 740], 'ymin': [359, 382, 305, 280, 377, 391, 342, 246, 308], 'xmax': [119, 283, 340, 393, 948, 853, 764, 631, 785], 'ymax': [395, 425, 331, 316, 418, 434, 373, 275, 341], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "--------------------\n",
      "{'filename': '/home/aiffel/aiffel/face_detector/widerface/WIDER_train/images/0--Parade/0_Parade_marchingband_1_778.jpg', 'width': 1024, 'height': 852, 'depth': 3, 'class': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'xmin': [27, 63, 64, 88, 231, 263, 367, 198, 293, 412, 441, 475, 510, 576, 577, 595, 570, 645, 719, 791, 884, 898, 945, 922, 743, 841, 980, 1001, 488, 586, 669, 744, 803, 294, 203], 'ymin': [226, 95, 63, 13, 1, 122, 68, 98, 161, 36, 23, 40, 23, 30, 71, 94, 126, 171, 98, 154, 97, 48, 89, 38, 71, 18, 56, 107, 2, 1, 1, 2, 3, 2, 0], 'xmax': [60, 79, 81, 104, 244, 277, 382, 213, 345, 426, 458, 489, 524, 592, 593, 611, 583, 697, 730, 845, 900, 913, 960, 937, 754, 857, 993, 1015, 500, 601, 681, 762, 821, 305, 216], 'ymax': [262, 114, 81, 28, 14, 142, 91, 116, 220, 56, 36, 61, 40, 45, 92, 114, 142, 229, 113, 203, 118, 69, 109, 54, 89, 34, 76, 120, 20, 18, 16, 17, 20, 12, 14], 'difficult': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "dataset_path = os.getenv('HOME')+'/aiffel/face_detector/widerface'\n",
    "anno_txt = 'wider_face_train_bbx_gt.txt'\n",
    "file_path = 'WIDER_train'\n",
    "for i, info in enumerate(parse_widerface(os.path.join(dataset_path, 'wider_face_split', anno_txt))):\n",
    "    print('--------------------')\n",
    "    image_file = os.path.join(dataset_path, file_path, 'images', info[0])\n",
    "    error, image_string, image_data = process_image(image_file)\n",
    "    boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "    print(boxes)\n",
    "    if i > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfrecord 생성\n",
    "\n",
    "# 1 개 데이터의 단위를 이루는 인스턴스를 생성하는 함수 생성\n",
    "\n",
    "def make_example(image_string, image_info_list):\n",
    "\n",
    "    for info in image_info_list:\n",
    "        filename = info['filename']\n",
    "        width = info['width']\n",
    "        height = info['height']\n",
    "        depth = info['depth']\n",
    "        classes = info['class']\n",
    "        xmin = info['xmin']\n",
    "        ymin = info['ymin']\n",
    "        xmax = info['xmax']\n",
    "        ymax = info['ymax']\n",
    "\n",
    "    if isinstance(image_string, type(tf.constant(0))):\n",
    "        encoded_image = [image_string.numpy()]\n",
    "    else:\n",
    "        encoded_image = [image_string]\n",
    "\n",
    "    base_name = [tf.compat.as_bytes(os.path.basename(filename))]\n",
    "\n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'filename':tf.train.Feature(bytes_list=tf.train.BytesList(value=base_name)),\n",
    "        'height':tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'width':tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'classes':tf.train.Feature(int64_list=tf.train.Int64List(value=classes)),\n",
    "        'x_mins':tf.train.Feature(float_list=tf.train.FloatList(value=xmin)),\n",
    "        'y_mins':tf.train.Feature(float_list=tf.train.FloatList(value=ymin)),\n",
    "        'x_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=xmax)),\n",
    "        'y_maxes':tf.train.Feature(float_list=tf.train.FloatList(value=ymax)),\n",
    "        'image_raw':tf.train.Feature(bytes_list=tf.train.BytesList(value=encoded_image))\n",
    "    }))\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12880/12880 [00:45<00:00, 281.99it/s]\n",
      "100%|██████████| 3226/3226 [00:11<00:00, 285.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터셋의 이미지파일, bounding box 를 파싱한 정보를 serialize하여 바이너리 파일로 생성\n",
    "\n",
    "import logging\n",
    "import tqdm\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "rootPath = os.getenv('HOME')+'/aiffel/face_detector'\n",
    "dataset_path = 'widerface'\n",
    "\n",
    "if not os.path.isdir(dataset_path):\n",
    "    logging.info('Please define valid dataset path.')\n",
    "else:\n",
    "    logging.info('Loading {}'.format(dataset_path))\n",
    "\n",
    "logging.info('Reading configuration...')\n",
    "\n",
    "for split in ['train', 'val']:\n",
    "    output_file = rootPath + '/dataset/train_mask.tfrecord' if split == 'train' else rootPath + '/dataset/val_mask.tfrecord'\n",
    "\n",
    "    with tf.io.TFRecordWriter(output_file) as writer:\n",
    "\n",
    "        counter = 0\n",
    "        skipped = 0\n",
    "        anno_txt = 'wider_face_train_bbx_gt.txt' if split == 'train' else 'wider_face_val_bbx_gt.txt'\n",
    "        file_path = 'WIDER_train' if split == 'train' else 'WIDER_val'\n",
    "        for info in tqdm.tqdm(parse_widerface(os.path.join(rootPath, dataset_path, 'wider_face_split', anno_txt))):\n",
    "            image_file = os.path.join(rootPath, dataset_path, file_path, 'images', info[0])\n",
    "\n",
    "            error, image_string, image_data = process_image(image_file)\n",
    "            boxes = xywh_to_voc(image_file, info[1], image_data)\n",
    "\n",
    "            if not error:\n",
    "                tf_example = make_example(image_string, [boxes])\n",
    "\n",
    "                writer.write(tf_example.SerializeToString())\n",
    "                counter += 1\n",
    "\n",
    "            else:\n",
    "                skipped += 1\n",
    "                logging.info('Skipped {:d} of {:d} images.'.format(skipped, counter))\n",
    "\n",
    "    logging.info('Wrote {} images to {}'.format(counter, output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 32,\n",
       " 'input_size': (240, 320),\n",
       " 'dataset_path': 'dataset/train_mask.tfrecord',\n",
       " 'val_path': 'dataset/val_mask.tfrecord',\n",
       " 'dataset_len': 12880,\n",
       " 'val_len': 3226,\n",
       " 'using_crop': True,\n",
       " 'using_bin': True,\n",
       " 'using_flip': True,\n",
       " 'using_distort': True,\n",
       " 'using_normalizing': True,\n",
       " 'labels_list': ['background', 'face'],\n",
       " 'min_sizes': [[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
       " 'steps': [8, 16, 32, 64],\n",
       " 'match_thresh': 0.45,\n",
       " 'variances': [0.1, 0.2],\n",
       " 'clip': False,\n",
       " 'base_channel': 16,\n",
       " 'resume': False,\n",
       " 'epoch': 100,\n",
       " 'init_lr': 0.01,\n",
       " 'lr_decay_epoch': [50, 70],\n",
       " 'lr_rate': 0.1,\n",
       " 'warmup_epoch': 5,\n",
       " 'min_lr': 0.0001,\n",
       " 'weights_decay': 0.0005,\n",
       " 'momentum': 0.9,\n",
       " 'save_freq': 10,\n",
       " 'score_threshold': 0.5,\n",
       " 'nms_threshold': 0.4,\n",
       " 'max_number_keep': 200}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 활용할 config 정보를 dict 로 정리\n",
    "\n",
    "cfg = {\n",
    "    # general setting\n",
    "    \"batch_size\": 32,\n",
    "    \"input_size\": (240, 320),  # (h,w)\n",
    "\n",
    "    # training dataset\n",
    "    \"dataset_path\": 'dataset/train_mask.tfrecord',  # 'dataset/trainval_mask.tfrecord'\n",
    "    \"val_path\": 'dataset/val_mask.tfrecord',  #\n",
    "    \"dataset_len\": 12880,  # train 6115 , trainval 7954, number of training samples\n",
    "    \"val_len\": 3226,\n",
    "    \"using_crop\": True,\n",
    "    \"using_bin\": True,\n",
    "    \"using_flip\": True,\n",
    "    \"using_distort\": True,\n",
    "    \"using_normalizing\": True,\n",
    "    \"labels_list\": ['background', 'face'],  # xml annotation\n",
    "\n",
    "    # anchor setting\n",
    "    \"min_sizes\":[[10, 16, 24], [32, 48], [64, 96], [128, 192, 256]],\n",
    "    \"steps\": [8, 16, 32, 64],\n",
    "    \"match_thresh\": 0.45,\n",
    "    \"variances\": [0.1, 0.2],\n",
    "    \"clip\": False,\n",
    "\n",
    "    # network\n",
    "    \"base_channel\": 16,\n",
    "\n",
    "    # training setting\n",
    "    \"resume\": False,  # if False,training from scratch\n",
    "    \"epoch\": 100,\n",
    "    \"init_lr\": 1e-2,\n",
    "    \"lr_decay_epoch\": [50, 70],\n",
    "    \"lr_rate\": 0.1,\n",
    "    \"warmup_epoch\": 5,\n",
    "    \"min_lr\": 1e-4,\n",
    "\n",
    "    \"weights_decay\": 5e-4,\n",
    "    \"momentum\": 0.9,\n",
    "    \"save_freq\": 10, #frequency of save model weights\n",
    "\n",
    "    # inference\n",
    "    \"score_threshold\": 0.5,\n",
    "    \"nms_threshold\": 0.4,\n",
    "    \"max_number_keep\": 200\n",
    "}\n",
    "\n",
    "cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 320)\n"
     ]
    }
   ],
   "source": [
    "# config 중 prior box 생성과 관련된 것들 추출\n",
    "\n",
    "image_sizes = cfg['input_size']\n",
    "min_sizes = cfg[\"min_sizes\"]\n",
    "steps = cfg[\"steps\"]\n",
    "clip = cfg[\"clip\"]\n",
    "\n",
    "if isinstance(image_sizes, int):\n",
    "    image_sizes = (image_sizes, image_sizes)\n",
    "elif isinstance(image_sizes, tuple):\n",
    "    image_sizes = image_sizes\n",
    "else:\n",
    "    raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "print(image_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[30, 40], [15, 20], [8, 10], [4, 5]]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prior box 생성을 위한 특성맵 생성\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "for m in range(4):\n",
    "    if (steps[m] != pow(2, (m + 3))):\n",
    "        print(\"steps must be [8,16,32,64]\")\n",
    "        sys.exit()\n",
    "\n",
    "assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "feature_maps = [\n",
    "    [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "    for step in steps]\n",
    "\n",
    "feature_maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17680"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 특성맵 별 prior box 생성\n",
    "\n",
    "anchors = []\n",
    "num_box_fm_cell=[]\n",
    "for k, f in enumerate(feature_maps):\n",
    "    num_box_fm_cell.append(len(min_sizes[k]))\n",
    "    for i, j in product(range(f[0]), range(f[1])):\n",
    "        for min_size in min_sizes[k]:\n",
    "            if isinstance(min_size, int):\n",
    "                min_size = (min_size, min_size)\n",
    "            elif isinstance(min_size, tuple):\n",
    "                min_size=min_size\n",
    "            else:\n",
    "                raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "            s_kx = min_size[1] / image_sizes[1]\n",
    "            s_ky = min_size[0] / image_sizes[0]\n",
    "            cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "            cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "            anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "len(anchors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4420, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prior box 형태 확인\n",
    "\n",
    "import numpy as np\n",
    "priors = np.asarray(anchors).reshape([-1, 4])\n",
    "priors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0125    , 0.01666667, 0.03125   , 0.04166667],\n",
       "       [0.0125    , 0.01666667, 0.05      , 0.06666667],\n",
       "       [0.0125    , 0.01666667, 0.075     , 0.1       ],\n",
       "       ...,\n",
       "       [0.9       , 0.93333333, 0.4       , 0.53333333],\n",
       "       [0.9       , 0.93333333, 0.6       , 0.8       ],\n",
       "       [0.9       , 0.93333333, 0.8       , 1.06666667]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prior 확인\n",
    "\n",
    "priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prior box 생성하는 함수 구현\n",
    "\n",
    "def prior_box(cfg,image_sizes=None):\n",
    "    \"\"\"prior box\"\"\"\n",
    "    if image_sizes is None:\n",
    "        image_sizes = cfg['input_size']\n",
    "    min_sizes=cfg[\"min_sizes\"]\n",
    "    steps=cfg[\"steps\"]\n",
    "    clip=cfg[\"clip\"]\n",
    "\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    for m in range(4):\n",
    "        if (steps[m] != pow(2, (m + 3))):\n",
    "            print(\"steps must be [8,16,32,64]\")\n",
    "            sys.exit()\n",
    "\n",
    "    assert len(min_sizes) == len(steps), \"anchors number didn't match the feature map layer.\"\n",
    "\n",
    "    feature_maps = [\n",
    "        [math.ceil(image_sizes[0] / step), math.ceil(image_sizes[1] / step)]\n",
    "        for step in steps]\n",
    "\n",
    "    anchors = []\n",
    "    num_box_fm_cell=[]\n",
    "    for k, f in enumerate(feature_maps):\n",
    "        num_box_fm_cell.append(len(min_sizes[k]))\n",
    "        for i, j in product(range(f[0]), range(f[1])):\n",
    "            for min_size in min_sizes[k]:\n",
    "                if isinstance(min_size, int):\n",
    "                    min_size = (min_size, min_size)\n",
    "                elif isinstance(min_size, tuple):\n",
    "                    min_size=min_size\n",
    "                else:\n",
    "                    raise Exception('Type error of min_sizes elements format,tuple or int. ')\n",
    "                s_kx = min_size[1] / image_sizes[1]\n",
    "                s_ky = min_size[0] / image_sizes[0]\n",
    "                cx = (j + 0.5) * steps[k] / image_sizes[1]\n",
    "                cy = (i + 0.5) * steps[k] / image_sizes[0]\n",
    "                anchors += [cx, cy, s_kx, s_ky]\n",
    "\n",
    "    output = np.asarray(anchors).reshape([-1, 4])\n",
    "\n",
    "    if clip:\n",
    "        output = np.clip(output, 0, 1)\n",
    "    return output,num_box_fm_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD 모델 내부에서 사용하는 레이어들 구현\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "def _conv_block(inputs, filters, kernel=(3, 3), strides=(1, 1), use_bn=True, padding=None, block_id=None):\n",
    "    \"\"\"Adds an initial convolution layer (with batch normalization and relu).\n",
    "    # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (2, 2):\n",
    "        x = tf.keras.layers.ZeroPadding2D(padding=((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='valid',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(x)\n",
    "    else:\n",
    "        x = tf.keras.layers.Conv2D(filters, kernel,\n",
    "                                   padding='same',\n",
    "                                   use_bias=False if use_bn else True,\n",
    "                                   strides=strides,\n",
    "                                   name='conv_%d' % block_id)(inputs)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_bn_%d' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_relu_%d' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _depthwise_conv_block(inputs, pointwise_conv_filters,\n",
    "                          depth_multiplier=1, strides=(1, 1), use_bn=True, block_id=None):\n",
    "    \"\"\"Adds a depthwise convolution block.\n",
    "        # Returns\n",
    "        Output tensor of block.\n",
    "    \"\"\"\n",
    "    if block_id is None:\n",
    "        block_id = (tf.keras.backend.get_uid())\n",
    "\n",
    "    if strides == (1, 1):\n",
    "        x = inputs\n",
    "    else:\n",
    "        x = tf.keras.layers.ZeroPadding2D(((1, 1), (1, 1)), name='conv_pad_%d' % block_id)(inputs)\n",
    "\n",
    "    x = tf.keras.layers.DepthwiseConv2D((3, 3),\n",
    "                                        padding='same' if strides == (1, 1) else 'valid',\n",
    "                                        depth_multiplier=depth_multiplier,\n",
    "                                        strides=strides,\n",
    "                                        use_bias=False if use_bn else True,\n",
    "                                        name='conv_dw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_dw_%d_bn' % block_id)(x)\n",
    "    x = tf.keras.layers.ReLU(name='conv_dw_%d_relu' % block_id)(x)\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(pointwise_conv_filters, (1, 1),\n",
    "                               padding='same',\n",
    "                               use_bias=False if use_bn else True,\n",
    "                               strides=(1, 1),\n",
    "                               name='conv_pw_%d' % block_id)(x)\n",
    "    if use_bn:\n",
    "        x = tf.keras.layers.BatchNormalization(name='conv_pw_%d_bn' % block_id)(x)\n",
    "    return tf.keras.layers.ReLU(name='conv_pw_%d_relu' % block_id)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _branch_block(input, filters):\n",
    "\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(input)\n",
    "    x = tf.keras.layers.LeakyReLU()(x)\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), padding='same')(x)\n",
    "\n",
    "    x1 = tf.keras.layers.Conv2D(filters * 2, kernel_size=(3, 3), padding='same')(input)\n",
    "\n",
    "    x = tf.keras.layers.Concatenate(axis=-1)([x, x1])\n",
    "\n",
    "    return tf.keras.layers.ReLU()(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_head_block(inputs, filters, strides=(1, 1), block_id=None):\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=(3, 3), strides=strides, padding='same')(inputs)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_heads(x, idx, num_class, num_cell):\n",
    "    \"\"\" Compute outputs of classification and regression heads\n",
    "    Args:\n",
    "        x: the input feature map\n",
    "        idx: index of the head layer\n",
    "    Returns:\n",
    "        conf: output of the idx-th classification head\n",
    "        loc: output of the idx-th regression head\n",
    "    \"\"\"\n",
    "    conf = _create_head_block(inputs=x, filters=num_cell[idx] * num_class)\n",
    "    conf = tf.keras.layers.Reshape((-1, num_class))(conf)\n",
    "    loc = _create_head_block(inputs=x, filters=num_cell[idx] * 4)\n",
    "    loc = tf.keras.layers.Reshape((-1, 4))(loc)\n",
    "\n",
    "    return conf, loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SSD Model 생성\n",
    "\n",
    "def SsdModel(cfg, num_cell, training=False, name='ssd_model'):\n",
    "    image_sizes = cfg['input_size']   if training else None\n",
    "    if isinstance(image_sizes, int):\n",
    "        image_sizes = (image_sizes, image_sizes)\n",
    "    elif isinstance(image_sizes, tuple):\n",
    "        image_sizes = image_sizes\n",
    "    elif image_sizes == None:\n",
    "        image_sizes = (None, None)\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    base_channel = cfg[\"base_channel\"]\n",
    "    num_class = len(cfg['labels_list'])\n",
    "\n",
    "    x = inputs = tf.keras.layers.Input(shape=[image_sizes[0], image_sizes[1], 3], name='input_image')\n",
    "\n",
    "    x = _conv_block(x, base_channel, strides=(2, 2))  # 120*160*16\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 2, strides=(2, 2))  # 60*80\n",
    "    x = _conv_block(x, base_channel * 2, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(2, 2))  # 30*40\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 4, strides=(1, 1))\n",
    "    x1 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _conv_block(x, base_channel * 8, strides=(2, 2))  # 15*20\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x = _conv_block(x, base_channel * 8, strides=(1, 1))\n",
    "    x2 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 8*10\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(1, 1))\n",
    "    x3 = _branch_block(x, base_channel)\n",
    "\n",
    "    x = _depthwise_conv_block(x, base_channel * 16, strides=(2, 2))  # 4*5\n",
    "    x4 = _branch_block(x, base_channel)\n",
    "\n",
    "    extra_layers = [x1, x2, x3, x4]\n",
    "\n",
    "    confs = []\n",
    "    locs = []\n",
    "\n",
    "    head_idx = 0\n",
    "    assert len(extra_layers) == len(num_cell)\n",
    "    for layer in extra_layers:\n",
    "        conf, loc = _compute_heads(layer, head_idx, num_class, num_cell)\n",
    "        confs.append(conf)\n",
    "        locs.append(loc)\n",
    "\n",
    "        head_idx += 1\n",
    "\n",
    "    confs = tf.keras.layers.Concatenate(axis=1, name=\"face_classes\")(confs)\n",
    "    locs = tf.keras.layers.Concatenate(axis=1, name=\"face_boxes\")(locs)\n",
    "\n",
    "    predictions = tf.keras.layers.Concatenate(axis=2, name='predictions')([locs, confs])\n",
    "\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=predictions, name=name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\n",
      "Model: \"ssd_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_1 (ZeroPadding2D)      (None, None, None, 3 0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_1 (Conv2D)                 (None, None, None, 1 432         conv_pad_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_1 (BatchNormalization)  (None, None, None, 1 64          conv_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_1 (ReLU)              (None, None, None, 1 0           conv_bn_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_2 (Conv2D)                 (None, None, None, 3 4608        conv_relu_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_2 (BatchNormalization)  (None, None, None, 3 128         conv_2[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_2 (ReLU)              (None, None, None, 3 0           conv_bn_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_3 (ZeroPadding2D)      (None, None, None, 3 0           conv_relu_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_3 (Conv2D)                 (None, None, None, 3 9216        conv_pad_3[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_3 (BatchNormalization)  (None, None, None, 3 128         conv_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_3 (ReLU)              (None, None, None, 3 0           conv_bn_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_4 (Conv2D)                 (None, None, None, 3 9216        conv_relu_3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_4 (BatchNormalization)  (None, None, None, 3 128         conv_4[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_4 (ReLU)              (None, None, None, 3 0           conv_bn_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_5 (ZeroPadding2D)      (None, None, None, 3 0           conv_relu_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_5 (Conv2D)                 (None, None, None, 6 18432       conv_pad_5[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_5 (BatchNormalization)  (None, None, None, 6 256         conv_5[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_5 (ReLU)              (None, None, None, 6 0           conv_bn_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_6 (Conv2D)                 (None, None, None, 6 36864       conv_relu_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_6 (BatchNormalization)  (None, None, None, 6 256         conv_6[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_6 (ReLU)              (None, None, None, 6 0           conv_bn_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_7 (Conv2D)                 (None, None, None, 6 36864       conv_relu_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_7 (BatchNormalization)  (None, None, None, 6 256         conv_7[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_7 (ReLU)              (None, None, None, 6 0           conv_bn_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_8 (Conv2D)                 (None, None, None, 6 36864       conv_relu_7[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_8 (BatchNormalization)  (None, None, None, 6 256         conv_8[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_8 (ReLU)              (None, None, None, 6 0           conv_bn_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_9 (ZeroPadding2D)      (None, None, None, 6 0           conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_9 (Conv2D)                 (None, None, None, 1 73728       conv_pad_9[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_9 (BatchNormalization)  (None, None, None, 1 512         conv_9[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_9 (ReLU)              (None, None, None, 1 0           conv_bn_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv_10 (Conv2D)                (None, None, None, 1 147456      conv_relu_9[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_10 (BatchNormalization) (None, None, None, 1 512         conv_10[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_10 (ReLU)             (None, None, None, 1 0           conv_bn_10[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_11 (Conv2D)                (None, None, None, 1 147456      conv_relu_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_11 (BatchNormalization) (None, None, None, 1 512         conv_11[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_11 (ReLU)             (None, None, None, 1 0           conv_bn_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_12 (ZeroPadding2D)     (None, None, None, 1 0           conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12 (DepthwiseConv2D)    (None, None, None, 1 1152        conv_pad_12[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12_bn (BatchNormalizati (None, None, None, 1 512         conv_dw_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_12_relu (ReLU)          (None, None, None, 1 0           conv_dw_12_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12 (Conv2D)             (None, None, None, 2 32768       conv_dw_12_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_12_relu (ReLU)          (None, None, None, 2 0           conv_pw_12_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pw_12_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_13_relu (ReLU)          (None, None, None, 2 0           conv_dw_13_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13 (Conv2D)             (None, None, None, 2 65536       conv_dw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_13_relu (ReLU)          (None, None, None, 2 0           conv_pw_13_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_14 (ZeroPadding2D)     (None, None, None, 2 0           conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14 (DepthwiseConv2D)    (None, None, None, 2 2304        conv_pad_14[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14_bn (BatchNormalizati (None, None, None, 2 1024        conv_dw_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_14_relu (ReLU)          (None, None, None, 2 0           conv_dw_14_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14 (Conv2D)             (None, None, None, 2 65536       conv_dw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14_bn (BatchNormalizati (None, None, None, 2 1024        conv_pw_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_14_relu (ReLU)          (None, None, None, 2 0           conv_pw_14_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, None, None, 1 9232        conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, None, None, 1 18448       conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, None, None, 1 36880       conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, None, None, 1 36880       conv_pw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, None, None, 1 0           conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, None, None, 1 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, None, None, 1 0           conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, None, None, 1 0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, None, None, 3 18464       conv_relu_8[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, None, None, 3 36896       conv_relu_11[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, None, None, 1 2320        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, None, None, 3 73760       conv_pw_13_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, None, None, 1 2320        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, None, None, 3 73760       conv_pw_14_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, None, None, 4 0           conv2d_1[0][0]                   \n",
      "                                                                 conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, None, None, 4 0           conv2d_4[0][0]                   \n",
      "                                                                 conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, None, None, 4 0           conv2d_7[0][0]                   \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, None, 4 0           conv2d_10[0][0]                  \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, None, None, 4 0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, None, None, 4 0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, None, None, 4 0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, None, None, 4 0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, None, None, 1 5196        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, None, None, 8 3464        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, None, None, 8 3464        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, None, None, 1 5196        re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, None, None, 6 2598        re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, None, None, 4 1732        re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, None, None, 4 1732        re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, None, None, 6 2598        re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)             (None, None, 4)      0           conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_3 (Reshape)             (None, None, 4)      0           conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_5 (Reshape)             (None, None, 4)      0           conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_7 (Reshape)             (None, None, 4)      0           conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, None, 2)      0           conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_2 (Reshape)             (None, None, 2)      0           conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_4 (Reshape)             (None, None, 2)      0           conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_6 (Reshape)             (None, None, 2)      0           conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_boxes (Concatenate)        (None, None, 4)      0           reshape_1[0][0]                  \n",
      "                                                                 reshape_3[0][0]                  \n",
      "                                                                 reshape_5[0][0]                  \n",
      "                                                                 reshape_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_classes (Concatenate)      (None, None, 2)      0           reshape[0][0]                    \n",
      "                                                                 reshape_2[0][0]                  \n",
      "                                                                 reshape_4[0][0]                  \n",
      "                                                                 reshape_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, None, 6)      0           face_boxes[0][0]                 \n",
      "                                                                 face_classes[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,038,956\n",
      "Trainable params: 1,034,636\n",
      "Non-trainable params: 4,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 생성한 SSD 모델 확인\n",
    "\n",
    "import os\n",
    "\n",
    "model = SsdModel(cfg, num_cell=[3, 2, 2, 3], training=False)\n",
    "print(len(model.layers))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에 augmentation 적용\n",
    "\n",
    "def _crop(img, labels, max_loop=250):\n",
    "    shape = tf.shape(img)\n",
    "\n",
    "    def matrix_iof(a, b):\n",
    "        \"\"\"\n",
    "        return iof of a and b, numpy version for data augenmentation\n",
    "        \"\"\"\n",
    "        lt = tf.math.maximum(a[:, tf.newaxis, :2], b[:, :2])\n",
    "        rb = tf.math.minimum(a[:, tf.newaxis, 2:], b[:, 2:])\n",
    "\n",
    "        area_i = tf.math.reduce_prod(rb - lt, axis=2) * \\\n",
    "            tf.cast(tf.reduce_all(lt < rb, axis=2), tf.float32)\n",
    "        area_a = tf.math.reduce_prod(a[:, 2:] - a[:, :2], axis=1)\n",
    "        return area_i / tf.math.maximum(area_a[:, tf.newaxis], 1)\n",
    "\n",
    "    def crop_loop_body(i, img, labels):\n",
    "        valid_crop = tf.constant(1, tf.int32)\n",
    "\n",
    "        pre_scale = tf.constant([0.3, 0.45, 0.6, 0.8, 1.0], dtype=tf.float32)\n",
    "        scale = pre_scale[tf.random.uniform([], 0, 5, dtype=tf.int32)]\n",
    "        short_side = tf.cast(tf.minimum(shape[0], shape[1]), tf.float32)\n",
    "        h = w = tf.cast(scale * short_side, tf.int32)\n",
    "        h_offset = tf.random.uniform([], 0, shape[0] - h + 1, dtype=tf.int32)\n",
    "        w_offset = tf.random.uniform([], 0, shape[1] - w + 1, dtype=tf.int32)\n",
    "        roi = tf.stack([w_offset, h_offset, w_offset + w, h_offset + h])\n",
    "        roi = tf.cast(roi, tf.float32)\n",
    "\n",
    "\n",
    "        value = matrix_iof(labels[:, :4], roi[tf.newaxis])\n",
    "        valid_crop = tf.cond(tf.math.reduce_any(value >= 1),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        centers = (labels[:, :2] + labels[:, 2:4]) / 2\n",
    "        mask_a = tf.reduce_all(\n",
    "            tf.math.logical_and(roi[:2] < centers, centers < roi[2:]),\n",
    "            axis=1)\n",
    "        labels_t = tf.boolean_mask(labels, mask_a)\n",
    "        valid_crop = tf.cond(tf.reduce_any(mask_a),\n",
    "                             lambda: valid_crop, lambda: 0)\n",
    "\n",
    "        img_t = img[h_offset:h_offset + h, w_offset:w_offset + w, :]\n",
    "        h_offset = tf.cast(h_offset, tf.float32)\n",
    "        w_offset = tf.cast(w_offset, tf.float32)\n",
    "        labels_t = tf.stack(\n",
    "            [labels_t[:, 0] - w_offset,  labels_t[:, 1] - h_offset,\n",
    "             labels_t[:, 2] - w_offset,  labels_t[:, 3] - h_offset,\n",
    "             labels_t[:, 4]], axis=1)\n",
    "\n",
    "        return tf.cond(valid_crop == 1,\n",
    "                       lambda: (max_loop, img_t, labels_t),\n",
    "                       lambda: (i + 1, img, labels))\n",
    "\n",
    "    _, img, labels = tf.while_loop(\n",
    "        lambda i, img, labels: tf.less(i, max_loop),\n",
    "        crop_loop_body,\n",
    "        [tf.constant(-1), img, labels],\n",
    "        shape_invariants=[tf.TensorShape([]),\n",
    "                          tf.TensorShape([None, None, 3]),\n",
    "                          tf.TensorShape([None, 5])])\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pad_to_square(img):\n",
    "    height = tf.shape(img)[0]\n",
    "    width = tf.shape(img)[1]\n",
    "\n",
    "    def pad_h():\n",
    "        img_pad_h = tf.ones([width - height, width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_h], axis=0)\n",
    "\n",
    "    def pad_w():\n",
    "        img_pad_w = tf.ones([height, height - width, 3]) * tf.reduce_mean(img, axis=[0, 1], keepdims=True)\n",
    "        return tf.concat([img, img_pad_w], axis=1)\n",
    "\n",
    "    img = tf.case([(tf.greater(height, width), pad_w),\n",
    "                   (tf.less(height, width), pad_h)], default=lambda: img)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resize(img, labels, img_dim):\n",
    "    ''' # resize and boxes coordinate to percent'''\n",
    "    w_f = tf.cast(tf.shape(img)[1], tf.float32)\n",
    "    h_f = tf.cast(tf.shape(img)[0], tf.float32)\n",
    "    locs = tf.stack([labels[:, 0] / w_f,  labels[:, 1] / h_f,\n",
    "                     labels[:, 2] / w_f,  labels[:, 3] / h_f] ,axis=1)\n",
    "    locs = tf.clip_by_value(locs, 0, 1.0)\n",
    "    labels = tf.concat([locs, labels[:, 4][:, tf.newaxis]], axis=1)\n",
    "\n",
    "    resize_case = tf.random.uniform([], 0, 5, dtype=tf.int32)\n",
    "    if isinstance(img_dim, int):\n",
    "        img_dim = (img_dim, img_dim)\n",
    "    elif isinstance(img_dim,tuple):\n",
    "        img_dim = img_dim\n",
    "    else:\n",
    "        raise Exception('Type error of input image size format,tuple or int. ')\n",
    "\n",
    "    def resize(method):\n",
    "        def _resize():\n",
    "            #　size h,w\n",
    "            return tf.image.resize(img, [img_dim[0], img_dim[1]], method=method, antialias=True)\n",
    "        return _resize\n",
    "\n",
    "    img = tf.case([(tf.equal(resize_case, 0), resize('bicubic')),\n",
    "                   (tf.equal(resize_case, 1), resize('area')),\n",
    "                   (tf.equal(resize_case, 2), resize('nearest')),\n",
    "                   (tf.equal(resize_case, 3), resize('lanczos3'))],\n",
    "                  default=resize('bilinear'))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _flip(img, labels):\n",
    "    flip_case = tf.random.uniform([], 0, 2, dtype=tf.int32)\n",
    "\n",
    "    def flip_func():\n",
    "        flip_img = tf.image.flip_left_right(img)\n",
    "        flip_labels = tf.stack([1 - labels[:, 2],  labels[:, 1],\n",
    "                                1 - labels[:, 0],  labels[:, 3],\n",
    "                                labels[:, 4]], axis=1)\n",
    "\n",
    "        return flip_img, flip_labels\n",
    "\n",
    "    img, labels = tf.case([(tf.equal(flip_case, 0), flip_func)],default=lambda: (img, labels))\n",
    "\n",
    "    return img, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distort(img):\n",
    "    img = tf.image.random_brightness(img, 0.4)\n",
    "    img = tf.image.random_contrast(img, 0.5, 1.5)\n",
    "    img = tf.image.random_saturation(img, 0.5, 1.5)\n",
    "    img = tf.image.random_hue(img, 0.1)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에 Prior box 적용\n",
    "\n",
    "def _intersect(box_a, box_b):\n",
    "    \"\"\" We resize both tensors to [A,B,2]:\n",
    "    [A,2] -> [A,1,2] -> [A,B,2]\n",
    "    [B,2] -> [1,B,2] -> [A,B,2]\n",
    "    Then we compute the area of intersect between box_a and box_b.\n",
    "    Args:\n",
    "      box_a: (tensor) bounding boxes, Shape: [A,4].\n",
    "      box_b: (tensor) bounding boxes, Shape: [B,4].\n",
    "    Return:\n",
    "      (tensor) intersection area, Shape: [A,B].\n",
    "    \"\"\"\n",
    "    A = tf.shape(box_a)[0]\n",
    "    B = tf.shape(box_b)[0]\n",
    "    max_xy = tf.minimum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, 2:], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, 2:], 0), [A, B, 2]))\n",
    "    min_xy = tf.maximum(\n",
    "        tf.broadcast_to(tf.expand_dims(box_a[:, :2], 1), [A, B, 2]),\n",
    "        tf.broadcast_to(tf.expand_dims(box_b[:, :2], 0), [A, B, 2]))\n",
    "    inter = tf.clip_by_value(max_xy - min_xy, 0.0, 512.0)\n",
    "    return inter[:, :, 0] * inter[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _jaccard(box_a, box_b):\n",
    "    \"\"\"Compute the jaccard overlap of two sets of boxes.  The jaccard overlap\n",
    "    is simply the intersection over union of two boxes.  Here we operate on\n",
    "    ground truth boxes and default boxes.\n",
    "    E.g.:\n",
    "        A ∩ B / A ∪ B = A ∩ B / (area(A) + area(B) - A ∩ B)\n",
    "    Args:\n",
    "        box_a: (tensor) Ground truth bounding boxes, Shape: [num_objects,4]\n",
    "        box_b: (tensor) Prior boxes from priorbox layers, Shape: [num_priors,4]\n",
    "    Return:\n",
    "        jaccard overlap: (tensor) Shape: [box_a.size(0), box_b.size(0)]\n",
    "    \"\"\"\n",
    "    inter = _intersect(box_a, box_b)\n",
    "    area_a = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_a[:, 2] - box_a[:, 0]) * (box_a[:, 3] - box_a[:, 1]), 1),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    area_b = tf.broadcast_to(\n",
    "        tf.expand_dims(\n",
    "            (box_b[:, 2] - box_b[:, 0]) * (box_b[:, 3] - box_b[:, 1]), 0),\n",
    "        tf.shape(inter))  # [A,B]\n",
    "    union = area_a + area_b - inter\n",
    "    return inter / union  # [A,B]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _encode_bbox(matched, priors, variances):\n",
    "    \"\"\"Encode the variances from the priorbox layers into the ground truth\n",
    "    boxes we have matched (based on jaccard overlap) with the prior boxes.\n",
    "    Args:\n",
    "        matched: (tensor) Coords of ground truth for each prior in point-form\n",
    "            Shape: [num_priors, 4].\n",
    "        priors: (tensor) Prior boxes in center-offset form\n",
    "            Shape: [num_priors,4].\n",
    "        variances: (list[float]) Variances of prior boxes\n",
    "    Return:\n",
    "        encoded boxes (tensor), Shape: [num_priors, 4]\n",
    "    \"\"\"\n",
    "    # dist b/t match center and prior's center\n",
    "    g_cxcy = (matched[:, :2] + matched[:, 2:]) / 2 - priors[:, :2]\n",
    "\n",
    "    # encode variance\n",
    "    g_cxcy /= (variances[0] * priors[:, 2:])\n",
    "\n",
    "    # match wh / prior wh\n",
    "    g_wh = (matched[:, 2:] - matched[:, :2]) / priors[:, 2:]\n",
    "    g_wh = tf.math.log(g_wh) / variances[1]\n",
    "\n",
    "    # return target for smooth_l1_loss\n",
    "    return tf.concat([g_cxcy, g_wh], 1)  # [num_priors,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tf(labels, priors, match_thresh, variances=None):\n",
    "    \"\"\"tensorflow encoding\"\"\"\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    priors = tf.cast(priors, tf.float32)\n",
    "    bbox = labels[:, :4]\n",
    "    conf = labels[:, -1]\n",
    "\n",
    "    # jaccard index\n",
    "    overlaps = _jaccard(bbox, priors)\n",
    "    best_prior_overlap = tf.reduce_max(overlaps, 1)\n",
    "    best_prior_idx = tf.argmax(overlaps, 1, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.reduce_max(overlaps, 0)\n",
    "    best_truth_idx = tf.argmax(overlaps, 0, tf.int32)\n",
    "\n",
    "    best_truth_overlap = tf.tensor_scatter_nd_update(\n",
    "        best_truth_overlap, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.ones_like(best_prior_idx, tf.float32) * 2.)\n",
    "    best_truth_idx = tf.tensor_scatter_nd_update(\n",
    "        best_truth_idx, tf.expand_dims(best_prior_idx, 1),\n",
    "        tf.range(tf.size(best_prior_idx), dtype=tf.int32))\n",
    "\n",
    "    # Scale Ground-Truth Boxes\n",
    "    matches_bbox = tf.gather(bbox, best_truth_idx)  # [num_priors, 4]\n",
    "    loc_t = _encode_bbox(matches_bbox, priors, variances)\n",
    "    conf_t = tf.gather(conf, best_truth_idx)  # [num_priors]\n",
    "    conf_t = tf.where(tf.less(best_truth_overlap, match_thresh), tf.zeros_like(conf_t), conf_t)\n",
    "\n",
    "    return tf.concat([loc_t, conf_t[..., tf.newaxis]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 생성\n",
    "\n",
    "def _transform_data(img_dim, using_crop,using_flip, using_distort, using_encoding,using_normalizing, priors,\n",
    "                    match_thresh,  variances):\n",
    "    def transform_data(img, labels):\n",
    "        img = tf.cast(img, tf.float32)\n",
    "        if using_crop:\n",
    "        # randomly crop\n",
    "            img, labels = _crop(img, labels)\n",
    "\n",
    "            # padding to square\n",
    "            img = _pad_to_square(img)\n",
    "\n",
    "        # resize and boxes coordinate to percent\n",
    "        img, labels = _resize(img, labels, img_dim)\n",
    "\n",
    "        # randomly left-right flip\n",
    "        if using_flip:\n",
    "            img, labels = _flip(img, labels)\n",
    "\n",
    "        # distort\n",
    "        if using_distort:\n",
    "            img = _distort(img)\n",
    "\n",
    "        # encode labels to feature targets\n",
    "        if using_encoding:\n",
    "            labels = encode_tf(labels=labels, priors=priors, match_thresh=match_thresh, variances=variances)\n",
    "        if using_normalizing:\n",
    "            img=(img/255.0-0.5)/1.0\n",
    "\n",
    "        return img, labels\n",
    "    return transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _parse_tfrecord(img_dim,using_crop, using_flip, using_distort,\n",
    "                    using_encoding, using_normalizing,priors, match_thresh,  variances):\n",
    "    def parse_tfrecord(tfrecord):\n",
    "        features = {\n",
    "            'filename': tf.io.FixedLenFeature([], tf.string),\n",
    "            'height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'classes': tf.io.VarLenFeature(tf.int64),\n",
    "            'x_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_mins': tf.io.VarLenFeature(tf.float32),\n",
    "            'x_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'y_maxes': tf.io.VarLenFeature(tf.float32),\n",
    "            'difficult':tf.io.VarLenFeature(tf.int64),\n",
    "            'image_raw': tf.io.FixedLenFeature([], tf.string),\n",
    "           }\n",
    "\n",
    "        parsed_example = tf.io.parse_single_example(tfrecord, features)\n",
    "        img = tf.image.decode_jpeg(parsed_example['image_raw'], channels=3)\n",
    "\n",
    "        width = tf.cast(parsed_example['width'], tf.float32)\n",
    "        height = tf.cast(parsed_example['height'], tf.float32)\n",
    "\n",
    "        labels = tf.sparse.to_dense(parsed_example['classes'])\n",
    "        labels = tf.cast(labels, tf.float32)\n",
    "\n",
    "        labels = tf.stack(\n",
    "            [tf.sparse.to_dense(parsed_example['x_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['y_mins']),\n",
    "             tf.sparse.to_dense(parsed_example['x_maxes']),\n",
    "             tf.sparse.to_dense(parsed_example['y_maxes']),labels], axis=1)\n",
    "\n",
    "        img, labels = _transform_data(\n",
    "            img_dim, using_crop,using_flip, using_distort, using_encoding, using_normalizing,priors,\n",
    "            match_thresh,  variances)(img, labels)\n",
    "\n",
    "        return img, labels\n",
    "    return parse_tfrecord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tfrecord_dataset(tfrecord_name, batch_size, img_dim,\n",
    "                          using_crop=True,using_flip=True, using_distort=True,\n",
    "                          using_encoding=True, using_normalizing=True,\n",
    "                          priors=None, match_thresh=0.45,variances=None,\n",
    "                          shuffle=True, repeat=True,buffer_size=10240):\n",
    "\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "\n",
    "    \"\"\"load dataset from tfrecord\"\"\"\n",
    "    if not using_encoding:\n",
    "        assert batch_size == 1\n",
    "    else:\n",
    "        assert priors is not None\n",
    "\n",
    "    raw_dataset = tf.data.TFRecordDataset(tfrecord_name)\n",
    "    raw_dataset = raw_dataset.cache()\n",
    "    if repeat:\n",
    "        raw_dataset = raw_dataset.repeat()\n",
    "    if shuffle:\n",
    "        raw_dataset = raw_dataset.shuffle(buffer_size=buffer_size)\n",
    "\n",
    "\n",
    "    dataset = raw_dataset.map(\n",
    "        _parse_tfrecord(img_dim, using_crop, using_flip, using_distort,\n",
    "                        using_encoding, using_normalizing,priors, match_thresh,  variances),\n",
    "        num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    dataset = dataset.batch(batch_size, drop_remainder=True)\n",
    "    dataset = dataset.prefetch(\n",
    "        buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(cfg, priors, shuffle=True, buffer_size=10240,train=True):\n",
    "    \"\"\"load dataset\"\"\"\n",
    "    global dataset\n",
    "    if train:\n",
    "        logging.info(\"load train dataset from {}\".format(cfg['dataset_path']))\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['dataset_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=cfg['using_crop'],\n",
    "            using_flip=cfg['using_flip'],\n",
    "            using_distort=cfg['using_distort'],\n",
    "            using_encoding=True,\n",
    "            using_normalizing=cfg['using_normalizing'],\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=True,\n",
    "            buffer_size=buffer_size)\n",
    "    else:\n",
    "        dataset = load_tfrecord_dataset(\n",
    "            tfrecord_name=os.path.join(rootPath, cfg['val_path']),\n",
    "            batch_size=cfg['batch_size'],\n",
    "            img_dim=cfg['input_size'],\n",
    "            using_crop=False,\n",
    "            using_flip=False,\n",
    "            using_distort=False,\n",
    "            using_encoding=True,\n",
    "            using_normalizing=True,\n",
    "            priors=priors,\n",
    "            match_thresh=cfg['match_thresh'],\n",
    "            variances=cfg['variances'],\n",
    "            shuffle=shuffle,\n",
    "            repeat=False,\n",
    "            buffer_size=buffer_size)\n",
    "        logging.info(\"load validation dataset from {}\".format(cfg['val_path']))\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate scheduler 설정\n",
    "\n",
    "class PiecewiseConstantWarmUpDecay(\n",
    "        tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\"A LearningRateSchedule wiht warm up schedule.\n",
    "    Modified from tf.keras.optimizers.schedules.PiecewiseConstantDecay\"\"\"\n",
    "\n",
    "    def __init__(self, boundaries, values, warmup_steps, min_lr,\n",
    "                 name=None):\n",
    "        super(PiecewiseConstantWarmUpDecay, self).__init__()\n",
    "\n",
    "        if len(boundaries) != len(values) - 1:\n",
    "            raise ValueError(\n",
    "                    \"The length of boundaries should be 1 less than the\"\n",
    "                    \"length of values\")\n",
    "\n",
    "        self.boundaries = boundaries\n",
    "        self.values = values\n",
    "        self.name = name\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.min_lr = min_lr\n",
    "\n",
    "    def __call__(self, step):\n",
    "        with tf.name_scope(self.name or \"PiecewiseConstantWarmUp\"):\n",
    "            step = tf.cast(tf.convert_to_tensor(step), tf.float32)\n",
    "            pred_fn_pairs = []\n",
    "            warmup_steps = self.warmup_steps\n",
    "            boundaries = self.boundaries\n",
    "            values = self.values\n",
    "            min_lr = self.min_lr\n",
    "\n",
    "            pred_fn_pairs.append(\n",
    "                (step <= warmup_steps,\n",
    "                 lambda: min_lr + step * (values[0] - min_lr) / warmup_steps))\n",
    "            pred_fn_pairs.append(\n",
    "                (tf.logical_and(step <= boundaries[0],\n",
    "                                step > warmup_steps),\n",
    "                 lambda: tf.constant(values[0])))\n",
    "            pred_fn_pairs.append(\n",
    "                (step > boundaries[-1], lambda: tf.constant(values[-1])))\n",
    "\n",
    "            for low, high, v in zip(boundaries[:-1], boundaries[1:],\n",
    "                                    values[1:-1]):\n",
    "                # Need to bind v here; can do this with lambda v=v: ...\n",
    "                pred = (step > low) & (step <= high)\n",
    "                pred_fn_pairs.append((pred, lambda: tf.constant(v)))\n",
    "\n",
    "            # The default isn't needed here because our conditions are mutually\n",
    "            # exclusive and exhaustive, but tf.case requires it.\n",
    "            return tf.case(pred_fn_pairs, lambda: tf.constant(values[0]),\n",
    "                           exclusive=True)\n",
    "\n",
    "    def get_config(self):\n",
    "        return {\n",
    "                \"boundaries\": self.boundaries,\n",
    "                \"values\": self.values,\n",
    "                \"warmup_steps\": self.warmup_steps,\n",
    "                \"min_lr\": self.min_lr,\n",
    "                \"name\": self.name\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiStepWarmUpLR(initial_learning_rate, lr_steps, lr_rate,\n",
    "                      warmup_steps=0., min_lr=0.,\n",
    "                      name='MultiStepWarmUpLR'):\n",
    "    \"\"\"Multi-steps warm up learning rate scheduler.\"\"\"\n",
    "    assert warmup_steps <= lr_steps[0]\n",
    "    assert min_lr <= initial_learning_rate\n",
    "    lr_steps_value = [initial_learning_rate]\n",
    "    for _ in range(len(lr_steps)):\n",
    "        lr_steps_value.append(lr_steps_value[-1] * lr_rate)\n",
    "    return PiecewiseConstantWarmUpDecay(\n",
    "        boundaries=lr_steps, values=lr_steps_value, warmup_steps=warmup_steps,\n",
    "        min_lr=min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# haed negative mining 적용\n",
    "\n",
    "def hard_negative_mining(loss, class_truth, neg_ratio):\n",
    "    \"\"\" Hard negative mining algorithm\n",
    "        to pick up negative examples for back-propagation\n",
    "        base on classification loss values\n",
    "    Args:\n",
    "        loss: list of classification losses of all default boxes (B, num_default)\n",
    "        class_truth: classification targets (B, num_default)\n",
    "        neg_ratio: negative / positive ratio\n",
    "    Returns:\n",
    "        class_loss: classification loss\n",
    "        loc_loss: regression loss\n",
    "    \"\"\"\n",
    "    # loss: B x N\n",
    "    # class_truth: B x N\n",
    "    pos_idx = class_truth > 0\n",
    "    num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.int32), axis=1)\n",
    "    num_neg = num_pos * neg_ratio\n",
    "\n",
    "    rank = tf.argsort(loss, axis=1, direction='DESCENDING')\n",
    "    rank = tf.argsort(rank, axis=1)\n",
    "    neg_idx = rank < tf.expand_dims(num_neg, 1)\n",
    "\n",
    "    return pos_idx, neg_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MultiBoxLoss(num_class=3, neg_pos_ratio=3.0):\n",
    "    def multi_loss(y_true, y_pred):\n",
    "        \"\"\" Compute losses for SSD\n",
    "               regression loss: smooth L1\n",
    "               classification loss: cross entropy\n",
    "           Args:\n",
    "               y_true: [B,N,5]\n",
    "               y_pred: [B,N,num_class]\n",
    "               class_pred: outputs of classification heads (B,N, num_classes)\n",
    "               loc_pred: outputs of regression heads (B,N, 4)\n",
    "               class_truth: classification targets (B,N)\n",
    "               loc_truth: regression targets (B,N, 4)\n",
    "           Returns:\n",
    "               class_loss: classification loss\n",
    "               loc_loss: regression loss\n",
    "       \"\"\"\n",
    "        num_batch = tf.shape(y_true)[0]\n",
    "        num_prior = tf.shape(y_true)[1]\n",
    "        loc_pred, class_pred = y_pred[..., :4], y_pred[..., 4:]\n",
    "        loc_truth, class_truth = y_true[..., :4], tf.squeeze(y_true[..., 4:])\n",
    "\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "        # compute classification losses without reduction\n",
    "        temp_loss = cross_entropy(class_truth, class_pred)\n",
    "        # 2. hard negative mining\n",
    "        pos_idx, neg_idx = hard_negative_mining(temp_loss, class_truth, neg_pos_ratio)\n",
    "\n",
    "        # classification loss will consist of positive and negative examples\n",
    "        cross_entropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='sum')\n",
    "\n",
    "        smooth_l1_loss = tf.keras.losses.Huber(reduction='sum')\n",
    "\n",
    "        loss_class = cross_entropy(\n",
    "            class_truth[tf.math.logical_or(pos_idx, neg_idx)],\n",
    "            class_pred[tf.math.logical_or(pos_idx, neg_idx)])\n",
    "\n",
    "        # localization loss only consist of positive examples (smooth L1)\n",
    "        loss_loc = smooth_l1_loss(loc_truth[pos_idx],loc_pred[pos_idx])\n",
    "\n",
    "        num_pos = tf.reduce_sum(tf.dtypes.cast(pos_idx, tf.float32))\n",
    "\n",
    "        loss_class = loss_class / num_pos\n",
    "        loss_loc = loss_loc / num_pos\n",
    "        return loss_loc, loss_class\n",
    "\n",
    "    return multi_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습\n",
    "\n",
    "global load_t1\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "logger = tf.get_logger()\n",
    "logger.disabled = True\n",
    "logger.setLevel(logging.FATAL)\n",
    "\n",
    "weights_dir = os.getenv('HOME')+'/aiffel/face_detector/checkpoints'\n",
    "if not os.path.exists(weights_dir):\n",
    "    os.mkdir(weights_dir)\n",
    "\n",
    "logging.info(\"Load configuration...\")\n",
    "label_classes = cfg['labels_list']\n",
    "logging.info(f\"Total image sample:{cfg['dataset_len']},Total classes number:\"\n",
    "             f\"{len(label_classes)},classes list:{label_classes}\")\n",
    "\n",
    "logging.info(\"Compute prior boxes...\")\n",
    "priors, num_cell = prior_box(cfg)\n",
    "logging.info(f\"Prior boxes number:{len(priors)},default anchor box number per feature map cell:{num_cell}\") # 4420, [3, 2, 2, 3]\n",
    "\n",
    "logging.info(\"Loading dataset...\")\n",
    "train_dataset = load_dataset(cfg, priors, shuffle=True, train=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"ssd_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_image (InputLayer)        [(None, 240, 320, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_15 (ZeroPadding2D)     (None, 242, 322, 3)  0           input_image[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_15 (Conv2D)                (None, 120, 160, 16) 432         conv_pad_15[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_15 (BatchNormalization) (None, 120, 160, 16) 64          conv_15[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_15 (ReLU)             (None, 120, 160, 16) 0           conv_bn_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_16 (Conv2D)                (None, 120, 160, 32) 4608        conv_relu_15[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_16 (BatchNormalization) (None, 120, 160, 32) 128         conv_16[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_16 (ReLU)             (None, 120, 160, 32) 0           conv_bn_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_17 (ZeroPadding2D)     (None, 122, 162, 32) 0           conv_relu_16[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_17 (Conv2D)                (None, 60, 80, 32)   9216        conv_pad_17[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_17 (BatchNormalization) (None, 60, 80, 32)   128         conv_17[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_17 (ReLU)             (None, 60, 80, 32)   0           conv_bn_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_18 (Conv2D)                (None, 60, 80, 32)   9216        conv_relu_17[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_18 (BatchNormalization) (None, 60, 80, 32)   128         conv_18[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_18 (ReLU)             (None, 60, 80, 32)   0           conv_bn_18[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_19 (ZeroPadding2D)     (None, 62, 82, 32)   0           conv_relu_18[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_19 (Conv2D)                (None, 30, 40, 64)   18432       conv_pad_19[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_19 (BatchNormalization) (None, 30, 40, 64)   256         conv_19[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_19 (ReLU)             (None, 30, 40, 64)   0           conv_bn_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_20 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_19[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_20 (BatchNormalization) (None, 30, 40, 64)   256         conv_20[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_20 (ReLU)             (None, 30, 40, 64)   0           conv_bn_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_21 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_21 (BatchNormalization) (None, 30, 40, 64)   256         conv_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_21 (ReLU)             (None, 30, 40, 64)   0           conv_bn_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_22 (Conv2D)                (None, 30, 40, 64)   36864       conv_relu_21[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_22 (BatchNormalization) (None, 30, 40, 64)   256         conv_22[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_22 (ReLU)             (None, 30, 40, 64)   0           conv_bn_22[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_23 (ZeroPadding2D)     (None, 32, 42, 64)   0           conv_relu_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_23 (Conv2D)                (None, 15, 20, 128)  73728       conv_pad_23[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_23 (BatchNormalization) (None, 15, 20, 128)  512         conv_23[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_23 (ReLU)             (None, 15, 20, 128)  0           conv_bn_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_24 (Conv2D)                (None, 15, 20, 128)  147456      conv_relu_23[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_24 (BatchNormalization) (None, 15, 20, 128)  512         conv_24[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_24 (ReLU)             (None, 15, 20, 128)  0           conv_bn_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_25 (Conv2D)                (None, 15, 20, 128)  147456      conv_relu_24[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_bn_25 (BatchNormalization) (None, 15, 20, 128)  512         conv_25[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv_relu_25 (ReLU)             (None, 15, 20, 128)  0           conv_bn_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_26 (ZeroPadding2D)     (None, 17, 22, 128)  0           conv_relu_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_26 (DepthwiseConv2D)    (None, 8, 10, 128)   1152        conv_pad_26[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_26_bn (BatchNormalizati (None, 8, 10, 128)   512         conv_dw_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_26_relu (ReLU)          (None, 8, 10, 128)   0           conv_dw_26_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_26 (Conv2D)             (None, 8, 10, 256)   32768       conv_dw_26_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_26_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_26[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_26_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_26_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_27 (DepthwiseConv2D)    (None, 8, 10, 256)   2304        conv_pw_26_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_27_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_dw_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_27_relu (ReLU)          (None, 8, 10, 256)   0           conv_dw_27_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_27 (Conv2D)             (None, 8, 10, 256)   65536       conv_dw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_27_bn (BatchNormalizati (None, 8, 10, 256)   1024        conv_pw_27[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_27_relu (ReLU)          (None, 8, 10, 256)   0           conv_pw_27_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pad_28 (ZeroPadding2D)     (None, 10, 12, 256)  0           conv_pw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_28 (DepthwiseConv2D)    (None, 4, 5, 256)    2304        conv_pad_28[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_28_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_dw_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_dw_28_relu (ReLU)          (None, 4, 5, 256)    0           conv_dw_28_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_28 (Conv2D)             (None, 4, 5, 256)    65536       conv_dw_28_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_28_bn (BatchNormalizati (None, 4, 5, 256)    1024        conv_pw_28[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_pw_28_relu (ReLU)          (None, 4, 5, 256)    0           conv_pw_28_bn[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 30, 40, 16)   9232        conv_relu_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 15, 20, 16)   18448       conv_relu_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 8, 10, 16)    36880       conv_pw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 4, 5, 16)     36880       conv_pw_28_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 30, 40, 16)   0           conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 15, 20, 16)   0           conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 8, 10, 16)    0           conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 4, 5, 16)     0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 30, 40, 16)   2320        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 30, 40, 32)   18464       conv_relu_22[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 15, 20, 16)   2320        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 15, 20, 32)   36896       conv_relu_25[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 8, 10, 16)    2320        leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 8, 10, 32)    73760       conv_pw_27_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 4, 5, 16)     2320        leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 4, 5, 32)     73760       conv_pw_28_relu[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 30, 40, 48)   0           conv2d_21[0][0]                  \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 15, 20, 48)   0           conv2d_24[0][0]                  \n",
      "                                                                 conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 8, 10, 48)    0           conv2d_27[0][0]                  \n",
      "                                                                 conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 4, 5, 48)     0           conv2d_30[0][0]                  \n",
      "                                                                 conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, 30, 40, 48)   0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, 15, 20, 48)   0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, 8, 10, 48)    0           concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, 4, 5, 48)     0           concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 30, 40, 12)   5196        re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 15, 20, 8)    3464        re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 8, 10, 8)     3464        re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 4, 5, 12)     5196        re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 30, 40, 6)    2598        re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 15, 20, 4)    1732        re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 8, 10, 4)     1732        re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 4, 5, 6)      2598        re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "reshape_9 (Reshape)             (None, 3600, 4)      0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 600, 4)       0           conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_13 (Reshape)            (None, 160, 4)       0           conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_15 (Reshape)            (None, 60, 4)        0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_8 (Reshape)             (None, 3600, 2)      0           conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_10 (Reshape)            (None, 600, 2)       0           conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_12 (Reshape)            (None, 160, 2)       0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "reshape_14 (Reshape)            (None, 60, 2)        0           conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "face_boxes (Concatenate)        (None, 4420, 4)      0           reshape_9[0][0]                  \n",
      "                                                                 reshape_11[0][0]                 \n",
      "                                                                 reshape_13[0][0]                 \n",
      "                                                                 reshape_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "face_classes (Concatenate)      (None, 4420, 2)      0           reshape_8[0][0]                  \n",
      "                                                                 reshape_10[0][0]                 \n",
      "                                                                 reshape_12[0][0]                 \n",
      "                                                                 reshape_14[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Concatenate)       (None, 4420, 6)      0           face_boxes[0][0]                 \n",
      "                                                                 face_classes[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 1,038,956\n",
      "Trainable params: 1,034,636\n",
      "Non-trainable params: 4,320\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Create Model...\")\n",
    "try:\n",
    "    model = SsdModel(cfg=cfg, num_cell=num_cell, training=True)\n",
    "    model.summary()\n",
    "    tf.keras.utils.plot_model(model, to_file=os.path.join(os.getcwd(), 'model.png'),\n",
    "                              show_shapes=True, show_layer_names=True)\n",
    "except Exception as e:\n",
    "    logging.error(e)\n",
    "    logging.info(\"Create network failed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg['resume']:\n",
    "    # Training from latest weights\n",
    "    paths = [os.path.join(weights_dir, path)\n",
    "             for path in os.listdir(weights_dir)]\n",
    "    latest = sorted(paths, key=os.path.getmtime)[-1]\n",
    "    model.load_weights(latest)\n",
    "    init_epoch = int(os.path.splitext(latest)[0][-3:])\n",
    "\n",
    "else:\n",
    "    init_epoch = -1\n",
    "\n",
    "steps_per_epoch = cfg['dataset_len'] // cfg['batch_size']\n",
    "logging.info(f\"steps_per_epoch:{steps_per_epoch}\")\n",
    "\n",
    "learning_rate = MultiStepWarmUpLR(\n",
    "    initial_learning_rate=cfg['init_lr'],\n",
    "    lr_steps=[e * steps_per_epoch for e in cfg['lr_decay_epoch']],\n",
    "    lr_rate=cfg['lr_rate'],\n",
    "    warmup_steps=cfg['warmup_epoch'] * steps_per_epoch,\n",
    "    min_lr=cfg['min_lr'])\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=cfg['momentum'], nesterov=True)\n",
    "multi_loss = MultiBoxLoss(num_class=len(label_classes), neg_pos_ratio=3)\n",
    "train_log_dir = 'logs/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        losses = {}\n",
    "        losses['reg'] = tf.reduce_sum(model.losses)  #unused. Init for redefine network\n",
    "        losses['loc'], losses['class'] = multi_loss(labels, predictions)\n",
    "        total_loss = tf.add_n([l for l in losses.values()])\n",
    "\n",
    "    grads = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    return total_loss, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100 | Batch 402/402 | Batch time 0.135 || Loss: 12.721543 | loc loss:6.250844 | class loss:6.470699  \n",
      "Epoch: 1/100  | Epoch time 155.274 || Average Loss: inf\n",
      "Epoch: 2/100 | Batch 402/402 | Batch time 0.113 || Loss: 7.992259 | loc loss:4.676844 | class loss:3.315414  \n",
      "Epoch: 2/100  | Epoch time 100.374 || Average Loss: inf\n",
      "Epoch: 3/100 | Batch 402/402 | Batch time 0.169 || Loss: 4.694567 | loc loss:2.522023 | class loss:2.172544  \n",
      "Epoch: 3/100  | Epoch time 101.572 || Average Loss: inf\n",
      "Epoch: 4/100 | Batch 402/402 | Batch time 0.168 || Loss: 6.402761 | loc loss:4.536559 | class loss:1.866203  \n",
      "Epoch: 4/100  | Epoch time 105.203 || Average Loss: inf\n",
      "Epoch: 5/100 | Batch 402/402 | Batch time 0.137 || Loss: 6.429219 | loc loss:4.488846 | class loss:1.940373  \n",
      "Epoch: 5/100  | Epoch time 104.338 || Average Loss: inf\n",
      "Epoch: 6/100 | Batch 402/402 | Batch time 0.085 || Loss: 5.169961 | loc loss:3.454072 | class loss:1.715890  \n",
      "Epoch: 6/100  | Epoch time 98.016 || Average Loss: inf\n",
      "Epoch: 7/100 | Batch 402/402 | Batch time 0.232 || Loss: 6.396181 | loc loss:4.452405 | class loss:1.943776 \n",
      "Epoch: 7/100  | Epoch time 90.517 || Average Loss: inf\n",
      "Epoch: 8/100 | Batch 402/402 | Batch time 0.161 || Loss: 4.344771 | loc loss:2.741874 | class loss:1.602896  \n",
      "Epoch: 8/100  | Epoch time 101.913 || Average Loss: inf\n",
      "Epoch: 9/100 | Batch 402/402 | Batch time 0.209 || Loss: 4.870445 | loc loss:3.349441 | class loss:1.521004 \n",
      "Epoch: 9/100  | Epoch time 103.988 || Average Loss: inf\n",
      "Epoch: 10/100 | Batch 402/402 | Batch time 0.180 || Loss: 6.157719 | loc loss:4.303628 | class loss:1.854091 \n",
      "Epoch: 10/100  | Epoch time 101.017 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel/aiffel/face_detector/checkpoints/weights_epoch_010.h5<<<<<<<<<<\n",
      "Epoch: 11/100 | Batch 402/402 | Batch time 0.130 || Loss: 6.364570 | loc loss:4.694846 | class loss:1.669724 \n",
      "Epoch: 11/100  | Epoch time 88.085 || Average Loss: inf\n",
      "Epoch: 12/100 | Batch 402/402 | Batch time 0.171 || Loss: 4.550462 | loc loss:3.083803 | class loss:1.466659 \n",
      "Epoch: 12/100  | Epoch time 90.562 || Average Loss: inf\n",
      "Epoch: 13/100 | Batch 402/402 | Batch time 0.200 || Loss: 5.143324 | loc loss:3.493496 | class loss:1.649828 \n",
      "Epoch: 13/100  | Epoch time 90.980 || Average Loss: inf\n",
      "Epoch: 14/100 | Batch 402/402 | Batch time 0.153 || Loss: 4.636804 | loc loss:3.219158 | class loss:1.417645 \n",
      "Epoch: 14/100  | Epoch time 91.106 || Average Loss: inf\n",
      "Epoch: 15/100 | Batch 402/402 | Batch time 0.141 || Loss: 5.286105 | loc loss:3.930120 | class loss:1.355985 \n",
      "Epoch: 15/100  | Epoch time 90.887 || Average Loss: inf\n",
      "Epoch: 16/100 | Batch 402/402 | Batch time 0.129 || Loss: 7.141027 | loc loss:5.541066 | class loss:1.599961 \n",
      "Epoch: 16/100  | Epoch time 96.088 || Average Loss: inf\n",
      "Epoch: 17/100 | Batch 402/402 | Batch time 0.158 || Loss: 4.947824 | loc loss:3.361393 | class loss:1.586432 \n",
      "Epoch: 17/100  | Epoch time 98.028 || Average Loss: inf\n",
      "Epoch: 18/100 | Batch 402/402 | Batch time 0.144 || Loss: 3.662117 | loc loss:2.401745 | class loss:1.260372 \n",
      "Epoch: 18/100  | Epoch time 101.350 || Average Loss: inf\n",
      "Epoch: 19/100 | Batch 402/402 | Batch time 0.120 || Loss: 5.347754 | loc loss:3.974574 | class loss:1.373180 \n",
      "Epoch: 19/100  | Epoch time 101.429 || Average Loss: inf\n",
      "Epoch: 20/100 | Batch 402/402 | Batch time 0.148 || Loss: 4.725952 | loc loss:3.515019 | class loss:1.210933 \n",
      "Epoch: 20/100  | Epoch time 99.373 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel/aiffel/face_detector/checkpoints/weights_epoch_020.h5<<<<<<<<<<\n",
      "Epoch: 21/100 | Batch 402/402 | Batch time 0.133 || Loss: 4.625667 | loc loss:3.159682 | class loss:1.465984 \n",
      "Epoch: 21/100  | Epoch time 100.146 || Average Loss: inf\n",
      "Epoch: 22/100 | Batch 402/402 | Batch time 0.199 || Loss: 4.361350 | loc loss:3.194717 | class loss:1.166633 \n",
      "Epoch: 22/100  | Epoch time 101.127 || Average Loss: inf\n",
      "Epoch: 23/100 | Batch 402/402 | Batch time 0.217 || Loss: 5.110634 | loc loss:3.772712 | class loss:1.337923 \n",
      "Epoch: 23/100  | Epoch time 100.746 || Average Loss: inf\n",
      "Epoch: 24/100 | Batch 402/402 | Batch time 0.190 || Loss: 6.078966 | loc loss:4.809178 | class loss:1.269788 \n",
      "Epoch: 24/100  | Epoch time 100.413 || Average Loss: inf\n",
      "Epoch: 25/100 | Batch 402/402 | Batch time 0.172 || Loss: 4.018529 | loc loss:2.427490 | class loss:1.591039 \n",
      "Epoch: 25/100  | Epoch time 101.532 || Average Loss: inf\n",
      "Epoch: 26/100 | Batch 402/402 | Batch time 0.132 || Loss: 4.568975 | loc loss:3.366201 | class loss:1.202774 \n",
      "Epoch: 26/100  | Epoch time 104.372 || Average Loss: inf\n",
      "Epoch: 27/100 | Batch 402/402 | Batch time 0.148 || Loss: 4.972302 | loc loss:3.951958 | class loss:1.020345 \n",
      "Epoch: 27/100  | Epoch time 101.181 || Average Loss: inf\n",
      "Epoch: 28/100 | Batch 402/402 | Batch time 0.335 || Loss: 6.416471 | loc loss:4.899278 | class loss:1.517193 \n",
      "Epoch: 28/100  | Epoch time 100.654 || Average Loss: inf\n",
      "Epoch: 29/100 | Batch 402/402 | Batch time 0.147 || Loss: 4.768936 | loc loss:3.380286 | class loss:1.388650 \n",
      "Epoch: 29/100  | Epoch time 100.079 || Average Loss: inf\n",
      "Epoch: 30/100 | Batch 402/402 | Batch time 0.162 || Loss: 4.946327 | loc loss:3.672672 | class loss:1.273655 \n",
      "Epoch: 30/100  | Epoch time 101.188 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel/aiffel/face_detector/checkpoints/weights_epoch_030.h5<<<<<<<<<<\n",
      "Epoch: 31/100 | Batch 402/402 | Batch time 0.146 || Loss: 4.526102 | loc loss:3.202815 | class loss:1.323287 \n",
      "Epoch: 31/100  | Epoch time 101.453 || Average Loss: inf\n",
      "Epoch: 32/100 | Batch 402/402 | Batch time 0.185 || Loss: 5.903062 | loc loss:4.512525 | class loss:1.390537 \n",
      "Epoch: 32/100  | Epoch time 101.224 || Average Loss: inf\n",
      "Epoch: 33/100 | Batch 402/402 | Batch time 0.224 || Loss: 5.134549 | loc loss:4.011627 | class loss:1.122922 \n",
      "Epoch: 33/100  | Epoch time 100.353 || Average Loss: inf\n",
      "Epoch: 34/100 | Batch 402/402 | Batch time 0.206 || Loss: 3.161857 | loc loss:1.966420 | class loss:1.195436 \n",
      "Epoch: 34/100  | Epoch time 100.631 || Average Loss: inf\n",
      "Epoch: 35/100 | Batch 402/402 | Batch time 0.224 || Loss: 4.003827 | loc loss:2.582316 | class loss:1.421511 \n",
      "Epoch: 35/100  | Epoch time 100.696 || Average Loss: inf\n",
      "Epoch: 36/100 | Batch 402/402 | Batch time 0.160 || Loss: 3.780550 | loc loss:2.473447 | class loss:1.307103  \n",
      "Epoch: 36/100  | Epoch time 99.977 || Average Loss: inf\n",
      "Epoch: 37/100 | Batch 402/402 | Batch time 0.144 || Loss: 4.608872 | loc loss:3.200477 | class loss:1.408395 \n",
      "Epoch: 37/100  | Epoch time 99.691 || Average Loss: inf\n",
      "Epoch: 38/100 | Batch 402/402 | Batch time 0.143 || Loss: 7.241825 | loc loss:5.814676 | class loss:1.427149 \n",
      "Epoch: 38/100  | Epoch time 101.142 || Average Loss: inf\n",
      "Epoch: 39/100 | Batch 402/402 | Batch time 0.134 || Loss: 4.707638 | loc loss:3.357737 | class loss:1.349900 \n",
      "Epoch: 39/100  | Epoch time 101.609 || Average Loss: inf\n",
      "Epoch: 40/100 | Batch 402/402 | Batch time 0.156 || Loss: 5.346975 | loc loss:3.953546 | class loss:1.393428 \n",
      "Epoch: 40/100  | Epoch time 100.346 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel/aiffel/face_detector/checkpoints/weights_epoch_040.h5<<<<<<<<<<\n",
      "Epoch: 41/100 | Batch 402/402 | Batch time 0.148 || Loss: 5.159083 | loc loss:4.060234 | class loss:1.098849 \n",
      "Epoch: 41/100  | Epoch time 102.595 || Average Loss: inf\n",
      "Epoch: 42/100 | Batch 402/402 | Batch time 0.165 || Loss: 4.956429 | loc loss:3.802209 | class loss:1.154219 \n",
      "Epoch: 42/100  | Epoch time 102.414 || Average Loss: inf\n",
      "Epoch: 43/100 | Batch 402/402 | Batch time 0.117 || Loss: 4.780157 | loc loss:3.375018 | class loss:1.405139 \n",
      "Epoch: 43/100  | Epoch time 101.663 || Average Loss: inf\n",
      "Epoch: 44/100 | Batch 402/402 | Batch time 0.278 || Loss: 3.856002 | loc loss:2.927636 | class loss:0.928366 \n",
      "Epoch: 44/100  | Epoch time 101.360 || Average Loss: inf\n",
      "Epoch: 45/100 | Batch 402/402 | Batch time 0.149 || Loss: 4.378726 | loc loss:3.428698 | class loss:0.950028 \n",
      "Epoch: 45/100  | Epoch time 103.229 || Average Loss: inf\n",
      "Epoch: 46/100 | Batch 402/402 | Batch time 0.173 || Loss: 4.736900 | loc loss:3.366779 | class loss:1.370121 \n",
      "Epoch: 46/100  | Epoch time 101.693 || Average Loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 47/100 | Batch 402/402 | Batch time 0.173 || Loss: 5.986012 | loc loss:4.421497 | class loss:1.564515 \n",
      "Epoch: 47/100  | Epoch time 100.198 || Average Loss: inf\n",
      "Epoch: 48/100 | Batch 402/402 | Batch time 0.142 || Loss: 3.685306 | loc loss:2.623301 | class loss:1.062004 \n",
      "Epoch: 48/100  | Epoch time 100.552 || Average Loss: inf\n",
      "Epoch: 49/100 | Batch 402/402 | Batch time 0.182 || Loss: 6.626969 | loc loss:5.034374 | class loss:1.592595 \n",
      "Epoch: 49/100  | Epoch time 104.370 || Average Loss: inf\n",
      "Epoch: 50/100 | Batch 402/402 | Batch time 0.149 || Loss: 5.049679 | loc loss:3.668031 | class loss:1.381648 \n",
      "Epoch: 50/100  | Epoch time 99.842 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel/aiffel/face_detector/checkpoints/weights_epoch_050.h5<<<<<<<<<<\n",
      "Epoch: 51/100 | Batch 402/402 | Batch time 0.186 || Loss: 5.362231 | loc loss:4.156492 | class loss:1.205739 \n",
      "Epoch: 51/100  | Epoch time 94.355 || Average Loss: inf\n",
      "Epoch: 52/100 | Batch 402/402 | Batch time 0.133 || Loss: 4.915430 | loc loss:4.008943 | class loss:0.906488 \n",
      "Epoch: 52/100  | Epoch time 89.049 || Average Loss: inf\n",
      "Epoch: 53/100 | Batch 402/402 | Batch time 0.168 || Loss: 4.264223 | loc loss:3.300332 | class loss:0.963892 \n",
      "Epoch: 53/100  | Epoch time 90.135 || Average Loss: inf\n",
      "Epoch: 54/100 | Batch 402/402 | Batch time 0.136 || Loss: 4.675219 | loc loss:3.340769 | class loss:1.334450 \n",
      "Epoch: 54/100  | Epoch time 89.052 || Average Loss: inf\n",
      "Epoch: 55/100 | Batch 402/402 | Batch time 0.188 || Loss: 5.629188 | loc loss:4.603603 | class loss:1.025585 \n",
      "Epoch: 55/100  | Epoch time 90.719 || Average Loss: inf\n",
      "Epoch: 56/100 | Batch 402/402 | Batch time 0.210 || Loss: 4.631586 | loc loss:3.414734 | class loss:1.216852 \n",
      "Epoch: 56/100  | Epoch time 93.097 || Average Loss: inf\n",
      "Epoch: 57/100 | Batch 402/402 | Batch time 0.210 || Loss: 5.523497 | loc loss:4.547760 | class loss:0.975737 \n",
      "Epoch: 57/100  | Epoch time 99.743 || Average Loss: inf\n",
      "Epoch: 58/100 | Batch 402/402 | Batch time 0.089 || Loss: 5.434463 | loc loss:4.263071 | class loss:1.171392 \n",
      "Epoch: 58/100  | Epoch time 102.638 || Average Loss: inf\n",
      "Epoch: 59/100 | Batch 402/402 | Batch time 0.221 || Loss: 8.132068 | loc loss:7.001555 | class loss:1.130512 \n",
      "Epoch: 59/100  | Epoch time 100.563 || Average Loss: inf\n",
      "Epoch: 60/100 | Batch 402/402 | Batch time 0.148 || Loss: 5.769287 | loc loss:4.527580 | class loss:1.241707 \n",
      "Epoch: 60/100  | Epoch time 99.291 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel/aiffel/face_detector/checkpoints/weights_epoch_060.h5<<<<<<<<<<\n",
      "Epoch: 61/100 | Batch 402/402 | Batch time 0.224 || Loss: 4.102937 | loc loss:2.902801 | class loss:1.200136 \n",
      "Epoch: 61/100  | Epoch time 99.330 || Average Loss: inf\n",
      "Epoch: 62/100 | Batch 402/402 | Batch time 0.174 || Loss: 4.428843 | loc loss:3.350756 | class loss:1.078088 \n",
      "Epoch: 62/100  | Epoch time 99.035 || Average Loss: inf\n",
      "Epoch: 63/100 | Batch 402/402 | Batch time 0.192 || Loss: 3.674227 | loc loss:2.553665 | class loss:1.120561 \n",
      "Epoch: 63/100  | Epoch time 101.181 || Average Loss: inf\n",
      "Epoch: 64/100 | Batch 402/402 | Batch time 0.225 || Loss: 3.144949 | loc loss:2.057560 | class loss:1.087389 \n",
      "Epoch: 64/100  | Epoch time 99.539 || Average Loss: inf\n",
      "Epoch: 65/100 | Batch 402/402 | Batch time 0.217 || Loss: 2.748493 | loc loss:1.568550 | class loss:1.179943 \n",
      "Epoch: 65/100  | Epoch time 98.992 || Average Loss: inf\n",
      "Epoch: 66/100 | Batch 402/402 | Batch time 0.253 || Loss: 3.825998 | loc loss:3.030775 | class loss:0.795223 \n",
      "Epoch: 66/100  | Epoch time 100.241 || Average Loss: inf\n",
      "Epoch: 67/100 | Batch 402/402 | Batch time 0.121 || Loss: 3.270451 | loc loss:2.241811 | class loss:1.028641 \n",
      "Epoch: 67/100  | Epoch time 101.014 || Average Loss: inf\n",
      "Epoch: 68/100 | Batch 402/402 | Batch time 0.128 || Loss: 4.512664 | loc loss:3.229075 | class loss:1.283589 \n",
      "Epoch: 68/100  | Epoch time 100.444 || Average Loss: inf\n",
      "Epoch: 69/100 | Batch 402/402 | Batch time 0.117 || Loss: 3.770629 | loc loss:2.960469 | class loss:0.810161 \n",
      "Epoch: 69/100  | Epoch time 101.002 || Average Loss: inf\n",
      "Epoch: 70/100 | Batch 402/402 | Batch time 0.197 || Loss: 4.463745 | loc loss:3.635895 | class loss:0.827850 \n",
      "Epoch: 70/100  | Epoch time 99.809 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel/aiffel/face_detector/checkpoints/weights_epoch_070.h5<<<<<<<<<<\n",
      "Epoch: 71/100 | Batch 402/402 | Batch time 0.163 || Loss: 5.130895 | loc loss:3.829189 | class loss:1.301706 \n",
      "Epoch: 71/100  | Epoch time 100.287 || Average Loss: inf\n",
      "Epoch: 72/100 | Batch 402/402 | Batch time 0.154 || Loss: 4.925655 | loc loss:3.861082 | class loss:1.064573 \n",
      "Epoch: 72/100  | Epoch time 100.085 || Average Loss: inf\n",
      "Epoch: 73/100 | Batch 402/402 | Batch time 0.148 || Loss: 3.850476 | loc loss:2.883102 | class loss:0.967374 \n",
      "Epoch: 73/100  | Epoch time 99.705 || Average Loss: inf\n",
      "Epoch: 74/100 | Batch 402/402 | Batch time 0.177 || Loss: 3.885290 | loc loss:2.805109 | class loss:1.080181 \n",
      "Epoch: 74/100  | Epoch time 100.087 || Average Loss: inf\n",
      "Epoch: 75/100 | Batch 402/402 | Batch time 0.177 || Loss: 3.356856 | loc loss:2.290351 | class loss:1.066505 \n",
      "Epoch: 75/100  | Epoch time 100.155 || Average Loss: inf\n",
      "Epoch: 76/100 | Batch 402/402 | Batch time 0.137 || Loss: 5.773131 | loc loss:4.476629 | class loss:1.296503 \n",
      "Epoch: 76/100  | Epoch time 100.266 || Average Loss: inf\n",
      "Epoch: 77/100 | Batch 402/402 | Batch time 0.146 || Loss: 5.499368 | loc loss:4.402453 | class loss:1.096914 \n",
      "Epoch: 77/100  | Epoch time 100.203 || Average Loss: inf\n",
      "Epoch: 78/100 | Batch 402/402 | Batch time 0.282 || Loss: 2.700614 | loc loss:1.927944 | class loss:0.772671 \n",
      "Epoch: 78/100  | Epoch time 99.984 || Average Loss: inf\n",
      "Epoch: 79/100 | Batch 402/402 | Batch time 0.107 || Loss: 4.868950 | loc loss:3.719810 | class loss:1.149140 \n",
      "Epoch: 79/100  | Epoch time 99.681 || Average Loss: inf\n",
      "Epoch: 80/100 | Batch 402/402 | Batch time 0.155 || Loss: 6.094656 | loc loss:4.990243 | class loss:1.104413 \n",
      "Epoch: 80/100  | Epoch time 100.531 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel/aiffel/face_detector/checkpoints/weights_epoch_080.h5<<<<<<<<<<\n",
      "Epoch: 81/100 | Batch 402/402 | Batch time 0.237 || Loss: 5.115701 | loc loss:4.090298 | class loss:1.025403 \n",
      "Epoch: 81/100  | Epoch time 99.855 || Average Loss: inf\n",
      "Epoch: 82/100 | Batch 402/402 | Batch time 0.205 || Loss: 6.294874 | loc loss:5.021226 | class loss:1.273648 \n",
      "Epoch: 82/100  | Epoch time 99.877 || Average Loss: inf\n",
      "Epoch: 83/100 | Batch 402/402 | Batch time 0.169 || Loss: 3.791809 | loc loss:2.700746 | class loss:1.091063 \n",
      "Epoch: 83/100  | Epoch time 101.571 || Average Loss: inf\n",
      "Epoch: 84/100 | Batch 402/402 | Batch time 0.185 || Loss: 4.571920 | loc loss:3.416901 | class loss:1.155019 \n",
      "Epoch: 84/100  | Epoch time 101.223 || Average Loss: inf\n",
      "Epoch: 85/100 | Batch 402/402 | Batch time 0.136 || Loss: 5.419032 | loc loss:4.381477 | class loss:1.037554 \n",
      "Epoch: 85/100  | Epoch time 101.450 || Average Loss: inf\n",
      "Epoch: 86/100 | Batch 402/402 | Batch time 0.131 || Loss: 5.209677 | loc loss:4.364735 | class loss:0.844942 \n",
      "Epoch: 86/100  | Epoch time 102.982 || Average Loss: inf\n",
      "Epoch: 87/100 | Batch 402/402 | Batch time 0.156 || Loss: 3.492028 | loc loss:2.354622 | class loss:1.137406 \n",
      "Epoch: 87/100  | Epoch time 102.247 || Average Loss: inf\n",
      "Epoch: 88/100 | Batch 402/402 | Batch time 0.149 || Loss: 3.848657 | loc loss:2.816456 | class loss:1.032201 \n",
      "Epoch: 88/100  | Epoch time 100.096 || Average Loss: inf\n",
      "Epoch: 89/100 | Batch 402/402 | Batch time 0.157 || Loss: 7.249073 | loc loss:6.000384 | class loss:1.248688 \n",
      "Epoch: 89/100  | Epoch time 100.752 || Average Loss: inf\n",
      "Epoch: 90/100 | Batch 402/402 | Batch time 0.228 || Loss: 4.047285 | loc loss:3.267768 | class loss:0.779517 \n",
      "Epoch: 90/100  | Epoch time 99.478 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel/aiffel/face_detector/checkpoints/weights_epoch_090.h5<<<<<<<<<<\n",
      "Epoch: 91/100 | Batch 402/402 | Batch time 0.114 || Loss: 6.871779 | loc loss:5.512827 | class loss:1.358952 \n",
      "Epoch: 91/100  | Epoch time 99.421 || Average Loss: inf\n",
      "Epoch: 92/100 | Batch 402/402 | Batch time 0.166 || Loss: 5.497420 | loc loss:4.601346 | class loss:0.896074 \n",
      "Epoch: 92/100  | Epoch time 100.554 || Average Loss: inf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 93/100 | Batch 402/402 | Batch time 0.250 || Loss: 3.559059 | loc loss:2.593358 | class loss:0.965701 \n",
      "Epoch: 93/100  | Epoch time 99.317 || Average Loss: 4.493803\n",
      "Epoch: 94/100 | Batch 402/402 | Batch time 0.172 || Loss: 5.431627 | loc loss:4.322361 | class loss:1.109266 \n",
      "Epoch: 94/100  | Epoch time 100.343 || Average Loss: inf\n",
      "Epoch: 95/100 | Batch 402/402 | Batch time 0.132 || Loss: 4.210255 | loc loss:3.241026 | class loss:0.969228 \n",
      "Epoch: 95/100  | Epoch time 101.266 || Average Loss: inf\n",
      "Epoch: 96/100 | Batch 402/402 | Batch time 0.200 || Loss: 4.835649 | loc loss:3.841391 | class loss:0.994258 \n",
      "Epoch: 96/100  | Epoch time 101.040 || Average Loss: inf\n",
      "Epoch: 97/100 | Batch 402/402 | Batch time 0.206 || Loss: 4.043577 | loc loss:2.909694 | class loss:1.133883 \n",
      "Epoch: 97/100  | Epoch time 99.252 || Average Loss: inf\n",
      "Epoch: 98/100 | Batch 402/402 | Batch time 0.129 || Loss: 4.209546 | loc loss:3.084111 | class loss:1.125435 \n",
      "Epoch: 98/100  | Epoch time 101.082 || Average Loss: inf\n",
      "Epoch: 99/100 | Batch 402/402 | Batch time 0.205 || Loss: 4.793343 | loc loss:3.676948 | class loss:1.116395 \n",
      "Epoch: 99/100  | Epoch time 100.657 || Average Loss: inf\n",
      "Epoch: 100/100 | Batch 402/402 | Batch time 0.208 || Loss: 4.581556 | loc loss:3.520909 | class loss:1.060647 \n",
      "Epoch: 100/100  | Epoch time 101.831 || Average Loss: inf\n",
      ">>>>>>>>>>Save weights file at /home/aiffel/aiffel/face_detector/checkpoints/weights_epoch_100.h5<<<<<<<<<<\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "for epoch in range(init_epoch+1,cfg['epoch']):\n",
    "    try:\n",
    "        start = time.time()\n",
    "        avg_loss = 0.0\n",
    "        for step, (inputs, labels) in enumerate(train_dataset.take(steps_per_epoch)):\n",
    "\n",
    "            load_t0 = time.time()\n",
    "            total_loss, losses = train_step(inputs, labels)\n",
    "            avg_loss = (avg_loss * step + total_loss.numpy()) / (step + 1)\n",
    "            load_t1 = time.time()\n",
    "            batch_time = load_t1 - load_t0\n",
    "\n",
    "            steps =steps_per_epoch*epoch+step\n",
    "            with train_summary_writer.as_default():\n",
    "                tf.summary.scalar('loss/total_loss', total_loss, step=steps)\n",
    "                for k, l in losses.items():\n",
    "                    tf.summary.scalar('loss/{}'.format(k), l, step=steps)\n",
    "                tf.summary.scalar('learning_rate', optimizer.lr(steps), step=steps)\n",
    "\n",
    "            print(f\"\\rEpoch: {epoch + 1}/{cfg['epoch']} | Batch {step + 1}/{steps_per_epoch} | Batch time {batch_time:.3f} || Loss: {total_loss:.6f} | loc loss:{losses['loc']:.6f} | class loss:{losses['class']:.6f} \",end = '',flush=True)\n",
    "\n",
    "        print(f\"\\nEpoch: {epoch + 1}/{cfg['epoch']}  | Epoch time {(load_t1 - start):.3f} || Average Loss: {avg_loss:.6f}\")\n",
    "\n",
    "        with train_summary_writer.as_default():\n",
    "            tf.summary.scalar('loss/avg_loss',avg_loss,step=epoch)\n",
    "\n",
    "        if (epoch + 1) % cfg['save_freq'] == 0:\n",
    "            filepath = os.path.join(weights_dir, f'weights_epoch_{(epoch + 1):03d}.h5')\n",
    "            model.save_weights(filepath)\n",
    "            if os.path.exists(filepath):\n",
    "                print(f\">>>>>>>>>>Save weights file at {filepath}<<<<<<<<<<\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print('interrupted')\n",
    "        exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMS 구현\n",
    "\n",
    "def decode_bbox_tf(pre, priors, variances=None):\n",
    "    \"\"\"Decode locations from predictions using prior to undo\n",
    "    the encoding we did for offset regression at train time.\n",
    "    Args:\n",
    "        pre (tensor): location predictions for loc layers,\n",
    "            Shape: [num_prior,4]\n",
    "        prior (tensor): Prior boxes in center-offset form.\n",
    "            Shape: [num_prior,4].\n",
    "        variances: (list[float]) Variances of prior boxes\n",
    "    Return:\n",
    "        decoded bounding box predictions xmin, ymin, xmax, ymax\n",
    "    \"\"\"\n",
    "    if variances is None:\n",
    "        variances = [0.1, 0.2]\n",
    "    centers = priors[:, :2] + pre[:, :2] * variances[0] * priors[:, 2:]\n",
    "    sides = priors[:, 2:] * tf.math.exp(pre[:, 2:] * variances[1])\n",
    "\n",
    "    return tf.concat([centers - sides / 2, centers + sides / 2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nms(boxes, scores, nms_threshold=0.5, limit=200):\n",
    "    \"\"\" Perform Non Maximum Suppression algorithm\n",
    "        to eliminate boxes with high overlap\n",
    "    Args:\n",
    "        boxes: tensor (num_boxes, 4)\n",
    "               of format (xmin, ymin, xmax, ymax)\n",
    "        scores: tensor (num_boxes,)\n",
    "        nms_threshold: NMS threshold\n",
    "        limit: maximum number of boxes to keep\n",
    "    Returns:\n",
    "        idx: indices of kept boxes\n",
    "    \"\"\"\n",
    "    if boxes.shape[0] == 0:\n",
    "        return tf.constant([], dtype=tf.int32)\n",
    "    selected = [0]\n",
    "    idx = tf.argsort(scores, direction='DESCENDING')\n",
    "    idx = idx[:limit]\n",
    "    boxes = tf.gather(boxes, idx)\n",
    "\n",
    "    iou = _jaccard(boxes, boxes)\n",
    "\n",
    "    while True:\n",
    "        row = iou[selected[-1]]\n",
    "        next_indices = row <= nms_threshold\n",
    "\n",
    "        iou = tf.where(\n",
    "            tf.expand_dims(tf.math.logical_not(next_indices), 0),\n",
    "            tf.ones_like(iou, dtype=tf.float32),\n",
    "            iou)\n",
    "\n",
    "        if not tf.math.reduce_any(next_indices):\n",
    "            break\n",
    "\n",
    "        selected.append(tf.argsort(\n",
    "            tf.dtypes.cast(next_indices, tf.int32), direction='DESCENDING')[0].numpy())\n",
    "\n",
    "    return tf.gather(idx, selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_predict(predictions, priors, cfg):\n",
    "    label_classes = cfg['labels_list']\n",
    "\n",
    "    bbox_regressions, confs = tf.split(predictions[0], [4, -1], axis=-1)\n",
    "    boxes = decode_bbox_tf(bbox_regressions, priors, cfg['variances'])\n",
    "\n",
    "\n",
    "    confs = tf.math.softmax(confs, axis=-1)\n",
    "\n",
    "    out_boxes = []\n",
    "    out_labels = []\n",
    "    out_scores = []\n",
    "\n",
    "    for c in range(1, len(label_classes)):\n",
    "        cls_scores = confs[:, c]\n",
    "\n",
    "        score_idx = cls_scores > cfg['score_threshold']\n",
    "\n",
    "        cls_boxes = boxes[score_idx]\n",
    "        cls_scores = cls_scores[score_idx]\n",
    "\n",
    "        nms_idx = compute_nms(cls_boxes, cls_scores, cfg['nms_threshold'], cfg['max_number_keep'])\n",
    "\n",
    "        cls_boxes = tf.gather(cls_boxes, nms_idx)\n",
    "        cls_scores = tf.gather(cls_scores, nms_idx)\n",
    "\n",
    "        cls_labels = [c] * cls_boxes.shape[0]\n",
    "\n",
    "        out_boxes.append(cls_boxes)\n",
    "        out_labels.extend(cls_labels)\n",
    "        out_scores.append(cls_scores)\n",
    "\n",
    "    out_boxes = tf.concat(out_boxes, axis=0)\n",
    "    out_scores = tf.concat(out_scores, axis=0)\n",
    "\n",
    "    boxes = tf.clip_by_value(out_boxes, 0.0, 1.0).numpy()\n",
    "    classes = np.array(out_labels)\n",
    "    scores = out_scores.numpy()\n",
    "\n",
    "    return boxes, classes, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사진에서 여러개의 얼굴 찾기\n",
    "\n",
    "def pad_input_image(img, max_steps):\n",
    "    \"\"\"pad image to suitable shape\"\"\"\n",
    "    img_h, img_w, _ = img.shape\n",
    "\n",
    "    img_pad_h = 0\n",
    "    if img_h % max_steps > 0:\n",
    "        img_pad_h = max_steps - img_h % max_steps\n",
    "\n",
    "    img_pad_w = 0\n",
    "    if img_w % max_steps > 0:\n",
    "        img_pad_w = max_steps - img_w % max_steps\n",
    "\n",
    "    padd_val = np.mean(img, axis=(0, 1)).astype(np.uint8)\n",
    "    img = cv2.copyMakeBorder(img, 0, img_pad_h, 0, img_pad_w,\n",
    "                             cv2.BORDER_CONSTANT, value=padd_val.tolist())\n",
    "    pad_params = (img_h, img_w, img_pad_h, img_pad_w)\n",
    "\n",
    "    return img, pad_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recover_pad_output(outputs, pad_params):\n",
    "    \"\"\"\n",
    "        recover the padded output effect\n",
    "\n",
    "    \"\"\"\n",
    "    img_h, img_w, img_pad_h, img_pad_w = pad_params\n",
    "\n",
    "    recover_xy = np.reshape(outputs[0], [-1, 2, 2]) * \\\n",
    "                 [(img_pad_w + img_w) / img_w, (img_pad_h + img_h) / img_h]\n",
    "    outputs[0] = np.reshape(recover_xy, [-1, 4])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(img, boxes, classes, scores, img_height, img_width, prior_index, class_list):\n",
    "    \"\"\"\n",
    "    draw bboxes and labels\n",
    "    out:boxes,classes,scores\n",
    "    \"\"\"\n",
    "    # bbox\n",
    "\n",
    "    x1, y1, x2, y2 = int(boxes[prior_index][0] * img_width), int(boxes[prior_index][1] * img_height), \\\n",
    "                     int(boxes[prior_index][2] * img_width), int(boxes[prior_index][3] * img_height)\n",
    "    if classes[prior_index] == 1:\n",
    "        color = (0, 255, 0)\n",
    "    else:\n",
    "        color = (0, 0, 255)\n",
    "    cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
    "    # confidence\n",
    "\n",
    "    # if scores:\n",
    "    #   score = \"{:.4f}\".format(scores[prior_index])\n",
    "    #   class_name = class_list[classes[prior_index]]\n",
    "\n",
    "    #  cv2.putText(img, '{} {}'.format(class_name, score),\n",
    "    #              (int(boxes[prior_index][0] * img_width), int(boxes[prior_index][1] * img_height) - 4),\n",
    "    #              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6e941a5cb2e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mglobal\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmin_sizes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'min_sizes'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnum_cell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_sizes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'HOME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/aiffel/face_detector/checkpoints'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "# 여러 사람의 얼굴이 포함된 테스트 이미지 선택 및 저장\n",
    "\n",
    "import cv2\n",
    "\n",
    "global model\n",
    "min_sizes = cfg['min_sizes']\n",
    "num_cell = [len(min_sizes[k]) for k in range(len(cfg['steps']))]\n",
    "model_path = os.getenv('HOME')+'/aiffel/face_detector/checkpoints'\n",
    "img_path = os.getenv('HOME')+'/aiffel/face_detector/image.png'\n",
    "\n",
    "try:\n",
    "    model = SsdModel(cfg=cfg, num_cell=num_cell, training=False)\n",
    "\n",
    "    paths = [os.path.join(model_path, path)\n",
    "             for path in os.listdir(model_path)]\n",
    "    latest = sorted(paths, key=os.path.getmtime)[-1]\n",
    "    model.load_weights(latest)\n",
    "    print(f\"model path : {latest}\")\n",
    "\n",
    "except AttributeError as e:\n",
    "    print('Please make sure there is at least one weights at {}'.format(model_path))\n",
    "\n",
    "if not os.path.exists(img_path):\n",
    "    print(f\"Cannot find image path from {img_path}\")\n",
    "    exit()\n",
    "print(\"[*] Predict {} image.. \".format(img_path))\n",
    "\n",
    "img_raw = cv2.imread(img_path)\n",
    "img_raw = cv2.resize(img_raw, (320, 240))\n",
    "img_height_raw, img_width_raw, _ = img_raw.shape\n",
    "img = np.float32(img_raw.copy())\n",
    "\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "# pad input image to avoid unmatched shape problem\n",
    "\n",
    "img, pad_params = pad_input_image(img, max_steps=max(cfg['steps']))\n",
    "img = img / 255.0 - 0.5\n",
    "print(img.shape)\n",
    "\n",
    "priors, _ = prior_box(cfg, image_sizes=(img.shape[0], img.shape[1]))\n",
    "priors = tf.cast(priors, tf.float32)\n",
    "\n",
    "predictions = model.predict(img[np.newaxis, ...])\n",
    "\n",
    "boxes, classes, scores = parse_predict(predictions, priors, cfg)\n",
    "\n",
    "print(f\"scores:{scores}\")\n",
    "\n",
    "\n",
    "# recover padding effect\n",
    "\n",
    "boxes = recover_pad_output(boxes, pad_params)\n",
    "\n",
    "\n",
    "# draw and save results\n",
    "\n",
    "save_img_path = os.path.join('assets/out_' + os.path.basename(img_path))\n",
    "\n",
    "for prior_index in range(len(boxes)):\n",
    "    show_image(img_raw, boxes, classes, scores, img_height_raw, img_width_raw, prior_index, cfg['labels_list'])\n",
    "\n",
    "cv2.imwrite(save_img_path, img_raw)\n",
    "cv2.imshow('results', img_raw)\n",
    "\n",
    "if cv2.waitKey(0) == ord('q'):\n",
    "    exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. dlib 을 이용한 landmark 찾기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "\n",
    "detector_hog = dlib.get_frontal_face_detector()\n",
    "landmark_predictor = dlib.shape_predictor('./model/shape_predictor_68_face_landmarks.dat')\n",
    "img_sticker = cv2.imread('./images/king.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img2sticker (img_orig, img_sticker, detector_hog, landmark_predictor):\n",
    "        \n",
    "    # preprocess\n",
    "    img_rgb = cv2.cvtColor(img_orig, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # detector\n",
    "    img_rgb_vga = cv2.resize(img_rgb, (640, 360))\n",
    "    dlib_rects = detector_hog(img_rgb_vga, 1) # <- 이미지 피라미드 수 변경\n",
    "    if len(dlib_rects) < 1:\n",
    "        return img_orig\n",
    "\n",
    "    # landmark\n",
    "    list_landmarks = []\n",
    "    for dlib_rect in dlib_rects:\n",
    "        points = landmark_predictor(img_rgb_vga, dlib_rect)\n",
    "        list_points = list(map(lambda p: (p.x, p.y), points.parts()))\n",
    "        list_landmarks.append(list_points)\n",
    "\n",
    "    # head coord\n",
    "    for dlib_rect, landmark in zip(dlib_rects, list_landmarks):\n",
    "        x = landmark[30][0] # nose\n",
    "        y = landmark[30][1] - dlib_rect.width()//2\n",
    "        w = dlib_rect.width()\n",
    "        h = dlib_rect.width()\n",
    "        break\n",
    "\n",
    "    # sticker\n",
    "    img_sticker = cv2.resize(img_sticker, (w,h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    refined_x = x - w // 2\n",
    "    refined_y = y - h\n",
    "\n",
    "    if refined_y < 0:\n",
    "        img_sticker = img_sticker[-refined_y:]\n",
    "        refined_y = 0\n",
    "        \n",
    "    if refined_x < 0:\n",
    "        img_sticker = img_sticker[:, -refined_x:]\n",
    "        refined_x = 0\n",
    "    \n",
    "    elif refined_x + img_sticker.shape[1] >= img_orig.shape[1]:\n",
    "        img_sticker = img_sticker[:, :-(img_sticker.shape[1]+refined_x-img_orig.shape[1])]\n",
    "\n",
    "    img_bgr = img_orig.copy()\n",
    "    sticker_area = img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x+img_sticker.shape[1]]\n",
    "\n",
    "    img_bgr[refined_y:refined_y+img_sticker.shape[0], refined_x:refined_x + img_sticker.shape[1]] = cv2.addWeighted(sticker_area, 1.0, img_sticker, 0.7, 0)\n",
    "\n",
    "    return img_bgr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 스티커 합성 사진 생성하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('./images/image.png')\n",
    "img_bgr = img2sticker(img, img_sticker, detector_hog, landmark_predictor)\n",
    "img_rgb = cv2.cvtColot(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "plt.imshow(img_rgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
