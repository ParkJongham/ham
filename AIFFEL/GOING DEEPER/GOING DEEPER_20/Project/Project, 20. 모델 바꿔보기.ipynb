{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project: 모델 바꿔보기\n",
    "\n",
    "## simplebaseline 모델로 변경해 봅시다\n",
    "\n",
    "지금까지 우리는 StackedHourglass Network 기반으로 학습을 진행해 왔습니다.\n",
    "\n",
    "그러나 지난시간에 살펴본 것처럼 Simplebaseline 모델은 이보다 훨씬 간단한 모델 구조에도 불구하고 더욱 좋은 성능을 보여주었던 바 있습니다.\n",
    "\n",
    "실제로도 그런 성능을 얻을 수 있을지 확인해 보겠습니다.\n",
    "\n",
    "## STEP 1 : simplebaseline 모델 완성하기\n",
    "\n",
    "`simplebaseline.py`  파일 내용을 완성합니다.\n",
    "\n",
    "## STEP 2 : simplebaseline 모델로 변경하여 훈련하기\n",
    "\n",
    "`train.py`  218 라인의 모델 선언 부분을 simplebaseline 모델로 변경한 후 다시 학습을 진행합니다.\n",
    "\n",
    "## STEP 3 : 두 모델의 비교\n",
    "\n",
    "실습에서 다룬 StackedHourglass Network 와 Simplebaseline 모델을 둘 다 동일한 Epoch 수만큼 학습하여 그 결과를 비교해 봅니다.\n",
    "\n",
    "-   Pose Estimation 결과 시각화 (정성적 비교)\n",
    "-   학습 진행경과 (loss 감소현황)\n",
    "\n",
    "가급적 두 모델 공히 최소 3 epoch이상, (5 epoch 이상 권장)을 학습하기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. simplebaseline 모델 완성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel/anaconda3/envs/aiffel/lib/python3.7/site-packages/ray/autoscaler/_private/cli_logger.py:61: FutureWarning: Not all Ray CLI dependencies were found. In Ray 1.4+, the Ray CLI, autoscaler, and dashboard will only be usable via `pip install 'ray[default]'`. Please update your install command.\n",
      "  \"update your install command.\", FutureWarning)\n",
      "2021-05-24 18:24:39,839\tINFO services.py:1269 -- View the Ray dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265\u001b[39m\u001b[22m\n"
     ]
    }
   ],
   "source": [
    "# tfrecord 만들기\n",
    "\n",
    "import csv\n",
    "import io\n",
    "import json\n",
    "import os\n",
    "import ray\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '-1' #CPU 사용\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "workdir = os.path.join(os.getenv('HOME'),'aiffel/mpii')\n",
    "os.chdir(workdir)\n",
    "\n",
    "from loguru import logger\n",
    "from PIL import Image\n",
    "import ray\n",
    "import tensorflow as tf\n",
    "\n",
    "num_train_shards = 64\n",
    "num_val_shards = 8\n",
    "ray.init()\n",
    "tf.get_logger().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"joints_vis\": [\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"joints\": [\n",
      "    [\n",
      "      620.0,\n",
      "      394.0\n",
      "    ],\n",
      "    [\n",
      "      616.0,\n",
      "      269.0\n",
      "    ],\n",
      "    [\n",
      "      573.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      188.0\n",
      "    ],\n",
      "    [\n",
      "      661.0,\n",
      "      221.0\n",
      "    ],\n",
      "    [\n",
      "      656.0,\n",
      "      231.0\n",
      "    ],\n",
      "    [\n",
      "      610.0,\n",
      "      187.0\n",
      "    ],\n",
      "    [\n",
      "      647.0,\n",
      "      176.0\n",
      "    ],\n",
      "    [\n",
      "      637.0201,\n",
      "      189.8183\n",
      "    ],\n",
      "    [\n",
      "      695.9799,\n",
      "      108.1817\n",
      "    ],\n",
      "    [\n",
      "      606.0,\n",
      "      217.0\n",
      "    ],\n",
      "    [\n",
      "      553.0,\n",
      "      161.0\n",
      "    ],\n",
      "    [\n",
      "      601.0,\n",
      "      167.0\n",
      "    ],\n",
      "    [\n",
      "      692.0,\n",
      "      185.0\n",
      "    ],\n",
      "    [\n",
      "      693.0,\n",
      "      240.0\n",
      "    ],\n",
      "    [\n",
      "      688.0,\n",
      "      313.0\n",
      "    ]\n",
      "  ],\n",
      "  \"image\": \"015601864.jpg\",\n",
      "  \"scale\": 3.021046,\n",
      "  \"center\": [\n",
      "    594.0,\n",
      "    257.0\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "json_file_path = os.getenv('HOME')+'/aiffel/mpii/mpii_human_pose_v1_u12_2/train.json'\n",
    "\n",
    "with open(json_file_path) as train_json:\n",
    "    train_annos = json.load(train_json)\n",
    "    json_formatted_str = json.dumps(train_annos[0], indent=2) # json beautify\n",
    "    print(json_formatted_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_one_annotation(anno, image_dir):\n",
    "    filename = anno['image']\n",
    "    joints = anno['joints']\n",
    "    joints_visibility = anno['joints_vis']\n",
    "    annotation = {\n",
    "        'filename': filename,\n",
    "        'filepath': os.path.join(image_dir, filename),\n",
    "        'joints_visibility': joints_visibility,\n",
    "        'joints': joints,\n",
    "        'center': anno['center'],\n",
    "        'scale' : anno['scale']\n",
    "    }\n",
    "    return annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_tf_records(annotations, total_shards, split):\n",
    "    chunks = chunkify(annotations, total_shards)\n",
    "    futures = [\n",
    "        # train_0001_of_0064.tfrecords\n",
    "        build_single_tfrecord.remote(\n",
    "            chunk, './tfrecords_mpii/{}_{}_of_{}.tfrecords'.format(\n",
    "                split,\n",
    "                str(i + 1).zfill(4),\n",
    "                str(total_shards).zfill(4),\n",
    "            )) for i, chunk in enumerate(chunks)\n",
    "    ]\n",
    "    ray.get(futures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunkify(l, n):\n",
    "    size = len(l) // n\n",
    "    start = 0\n",
    "    results = []\n",
    "    for i in range(n - 1):\n",
    "        results.append(l[start:start + size])\n",
    "        start += size\n",
    "    results.append(l[start:])\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def build_single_tfrecord(chunk, path):\n",
    "    print('start to build tf records for ' + path)\n",
    "\n",
    "    with tf.io.TFRecordWriter(path) as writer:\n",
    "        for anno_list in chunk:\n",
    "            tf_example = generate_tfexample(anno_list)\n",
    "            writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    print('finished building tf records for ' + path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tfexample(anno):\n",
    "    filename = anno['filename']\n",
    "    filepath = anno['filepath']\n",
    "    with open(filepath, 'rb') as image_file:\n",
    "        content = image_file.read()\n",
    "\n",
    "    image = Image.open(filepath)\n",
    "    if image.format != 'JPEG' or image.mode != 'RGB':\n",
    "        image_rgb = image.convert('RGB')\n",
    "        with io.BytesIO() as output:\n",
    "            image_rgb.save(output, format=\"JPEG\", quality=95)\n",
    "            content = output.getvalue()\n",
    "\n",
    "    width, height = image.size\n",
    "    depth = 3\n",
    "\n",
    "    c_x = int(anno['center'][0])\n",
    "    c_y = int(anno['center'][1])\n",
    "    scale = anno['scale']\n",
    "\n",
    "    x = [\n",
    "        int(joint[0]) if joint[0] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    y = [\n",
    "        int(joint[1]) if joint[1] >= 0 else int(joint[0]) \n",
    "        for joint in anno['joints']\n",
    "    ]\n",
    "    # 0 - invisible, 1 - occluded, 2 - visible\n",
    "    v = [0 if joint_v == 0 else 2 for joint_v in anno['joints_visibility']]\n",
    "\n",
    "    feature = {\n",
    "        'image/height':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[height])),\n",
    "        'image/width':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[width])),\n",
    "        'image/depth':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[depth])),\n",
    "        'image/object/parts/x':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=x)),\n",
    "        'image/object/parts/y':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=y)),\n",
    "        'image/object/center/x': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_x])),\n",
    "        'image/object/center/y': \n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=[c_y])),\n",
    "        'image/object/scale':\n",
    "        tf.train.Feature(float_list=tf.train.FloatList(value=[scale])),\n",
    "        'image/object/parts/v':\n",
    "        tf.train.Feature(int64_list=tf.train.Int64List(value=v)),\n",
    "        'image/encoded':\n",
    "        _bytes_feature(content),\n",
    "        'image/filename':\n",
    "        _bytes_feature(filename.encode())\n",
    "    }\n",
    "\n",
    "    return tf.train.Example(features=tf.train.Features(feature=feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _bytes_feature(value):\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy(\n",
    "        )  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print('Start to parse annotations.')\n",
    "    if not os.path.exists('./tfrecords_mpii'):\n",
    "        os.makedirs('./tfrecords_mpii')\n",
    "\n",
    "    with open('./mpii_human_pose_v1_u12_2/train.json') as train_json:\n",
    "        train_annos = json.load(train_json)\n",
    "        train_annotations = [\n",
    "            parse_one_annotation(anno, './images/')\n",
    "            for anno in train_annos\n",
    "        ]\n",
    "        print('First train annotation: ', train_annotations[0])\n",
    "        del (train_annos)\n",
    "\n",
    "    with open('./mpii_human_pose_v1_u12_2/validation.json') as val_json:\n",
    "        val_annos = json.load(val_json)\n",
    "        val_annotations = [\n",
    "            parse_one_annotation(anno, './images/') for anno in val_annos\n",
    "        ]\n",
    "        print('First val annotation: ', val_annotations[0])\n",
    "        del (val_annos)\n",
    "\n",
    "    print('Start to build TF Records.')\n",
    "    build_tf_records(train_annotations, num_train_shards, 'train')\n",
    "    build_tf_records(val_annotations, num_val_shards, 'val')\n",
    "\n",
    "    print('Successfully wrote {} annotations to TF Records.'.format(\n",
    "        len(train_annotations) + len(val_annotations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start to parse annotations.\n",
      "First train annotation:  {'filename': '015601864.jpg', 'filepath': './images/015601864.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[620.0, 394.0], [616.0, 269.0], [573.0, 185.0], [647.0, 188.0], [661.0, 221.0], [656.0, 231.0], [610.0, 187.0], [647.0, 176.0], [637.0201, 189.8183], [695.9799, 108.1817], [606.0, 217.0], [553.0, 161.0], [601.0, 167.0], [692.0, 185.0], [693.0, 240.0], [688.0, 313.0]], 'center': [594.0, 257.0], 'scale': 3.021046}\n",
      "First val annotation:  {'filename': '005808361.jpg', 'filepath': './images/005808361.jpg', 'joints_visibility': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'joints': [[804.0, 711.0], [816.0, 510.0], [908.0, 438.0], [1040.0, 454.0], [906.0, 528.0], [883.0, 707.0], [974.0, 446.0], [985.0, 253.0], [982.7591, 235.9694], [962.2409, 80.0306], [869.0, 214.0], [798.0, 340.0], [902.0, 253.0], [1067.0, 253.0], [1167.0, 353.0], [1142.0, 478.0]], 'center': [966.0, 340.0], 'scale': 4.718488}\n",
      "Start to build TF Records.\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30564)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30566)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30567)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30568)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0002_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0003_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0004_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0001_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0007_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30564)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0009_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30564)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30566)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0005_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30566)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30567)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0008_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30567)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0010_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30568)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0006_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30568)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0012_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0011_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0013_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0014_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0015_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0016_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30568)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0022_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30568)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0017_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30567)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0020_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30567)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30564)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0018_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30564)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0023_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30566)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0019_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30566)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0024_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0025_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0021_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0026_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0027_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0028_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30568)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0029_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30568)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30566)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0034_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30566)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0033_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0037_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30564)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0032_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30564)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0030_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30567)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0031_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30567)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0038_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0040_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0039_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0035_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0036_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30568)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0041_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30568)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30566)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0042_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30566)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30567)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0047_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30567)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30564)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0045_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30564)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0050_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0043_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0049_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0044_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0046_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0052_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0048_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0051_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m start to build tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30568)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0053_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30566)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0054_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30567)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0055_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30564)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0056_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0058_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0063_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0059_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0060_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0057_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0061_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0062_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m finished building tf records for ./tfrecords_mpii/train_0064_of_0064.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m start to build tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30572)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0005_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30561)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0008_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30562)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0006_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30569)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0007_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30571)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0003_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30565)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0001_of_0008.tfrecords\n",
      "Successfully wrote 25204 annotations to TF Records.\n",
      "\u001b[2m\u001b[36m(pid=30570)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0002_of_0008.tfrecords\n",
      "\u001b[2m\u001b[36m(pid=30563)\u001b[0m finished building tf records for ./tfrecords_mpii/val_0004_of_0008.tfrecords\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data label 로 만들기\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class Preprocessor(object):\n",
    "    def __init__(self,\n",
    "                 image_shape=(256, 256, 3),\n",
    "                 heatmap_shape=(64, 64, 16),\n",
    "                 is_train=False):\n",
    "        self.is_train = is_train\n",
    "        self.image_shape = image_shape\n",
    "        self.heatmap_shape = heatmap_shape\n",
    "\n",
    "    def __call__(self, example):\n",
    "        features = self.parse_tfexample(example)\n",
    "        image = tf.io.decode_jpeg(features['image/encoded'])\n",
    "\n",
    "        if self.is_train:\n",
    "            random_margin = tf.random.uniform([1], 0.1, 0.3)[0]\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features, margin=random_margin)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "        else:\n",
    "            image, keypoint_x, keypoint_y = self.crop_roi(image, features)\n",
    "            image = tf.image.resize(image, self.image_shape[0:2])\n",
    "\n",
    "        image = tf.cast(image, tf.float32) / 127.5 - 1\n",
    "        heatmaps = self.make_heatmaps(features, keypoint_x, keypoint_y)\n",
    "\n",
    "        # print (image.shape, heatmaps.shape, type(heatmaps))\n",
    "\n",
    "        return image, heatmaps\n",
    "\n",
    "\n",
    "    def crop_roi(self, image, features, margin=0.2):\n",
    "        img_shape = tf.shape(image)\n",
    "        img_height = img_shape[0]\n",
    "        img_width = img_shape[1]\n",
    "        img_depth = img_shape[2]\n",
    "\n",
    "        keypoint_x = tf.cast(tf.sparse.to_dense(features['image/object/parts/x']), dtype=tf.int32)\n",
    "        keypoint_y = tf.cast(tf.sparse.to_dense(features['image/object/parts/y']), dtype=tf.int32)\n",
    "        center_x = features['image/object/center/x']\n",
    "        center_y = features['image/object/center/y']\n",
    "        body_height = features['image/object/scale'] * 200.0\n",
    "\n",
    "        masked_keypoint_x = tf.boolean_mask(keypoint_x, keypoint_x > 0)\n",
    "        masked_keypoint_y = tf.boolean_mask(keypoint_y, keypoint_y > 0)\n",
    "\n",
    "        keypoint_xmin = tf.reduce_min(masked_keypoint_x)\n",
    "        keypoint_xmax = tf.reduce_max(masked_keypoint_x)\n",
    "        keypoint_ymin = tf.reduce_min(masked_keypoint_y)\n",
    "        keypoint_ymax = tf.reduce_max(masked_keypoint_y)\n",
    "\n",
    "        xmin = keypoint_xmin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        xmax = keypoint_xmax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymin = keypoint_ymin - tf.cast(body_height * margin, dtype=tf.int32)\n",
    "        ymax = keypoint_ymax + tf.cast(body_height * margin, dtype=tf.int32)\n",
    "\n",
    "        effective_xmin = xmin if xmin > 0 else 0\n",
    "        effective_ymin = ymin if ymin > 0 else 0\n",
    "        effective_xmax = xmax if xmax < img_width else img_width\n",
    "        effective_ymax = ymax if ymax < img_height else img_height\n",
    "        effective_height = effective_ymax - effective_ymin\n",
    "        effective_width = effective_xmax - effective_xmin\n",
    "\n",
    "        image = image[effective_ymin:effective_ymax, effective_xmin:effective_xmax, :]\n",
    "        new_shape = tf.shape(image)\n",
    "        new_height = new_shape[0]\n",
    "        new_width = new_shape[1]\n",
    "\n",
    "        effective_keypoint_x = (keypoint_x - effective_xmin) / new_width\n",
    "        effective_keypoint_y = (keypoint_y - effective_ymin) / new_height\n",
    "\n",
    "        return image, effective_keypoint_x, effective_keypoint_y\n",
    "\n",
    "\n",
    "    def generate_2d_guassian(self, height, width, y0, x0, visibility=2, sigma=1, scale=12):\n",
    "        \"\"\"\n",
    "        \"The same technique as Tompson et al. is used for supervision. A MeanSquared Error (MSE) loss is\n",
    "        applied comparing the predicted heatmap to a ground-truth heatmap consisting of a 2D gaussian\n",
    "        (with standard deviation of 1 px) centered on the keypoint location.\"\n",
    "\n",
    "        https://github.com/princeton-vl/pose-hg-train/blob/master/src/util/img.lua#L204\n",
    "        \"\"\"\n",
    "        heatmap = tf.zeros((height, width))\n",
    "\n",
    "        # this gaussian patch is 7x7, let's get four corners of it first\n",
    "        xmin = x0 - 3 * sigma\n",
    "        ymin = y0 - 3 * sigma\n",
    "        xmax = x0 + 3 * sigma\n",
    "        ymax = y0 + 3 * sigma\n",
    "        # if the patch is out of image boundary we simply return nothing according to the source code\n",
    "        # [1]\"In these cases the joint is either truncated or severely occluded, so for\n",
    "        # supervision a ground truth heatmap of all zeros is provided.\"\n",
    "        if xmin >= width or ymin >= height or xmax < 0 or ymax <0 or visibility == 0:\n",
    "            return heatmap\n",
    "\n",
    "        size = 6 * sigma + 1\n",
    "        x, y = tf.meshgrid(tf.range(0, 6*sigma+1, 1), tf.range(0, 6*sigma+1, 1), indexing='xy')\n",
    "\n",
    "        # the center of the gaussian patch should be 1\n",
    "        center_x = size // 2\n",
    "        center_y = size // 2\n",
    "\n",
    "        # generate this 7x7 gaussian patch\n",
    "        gaussian_patch = tf.cast(tf.math.exp(-(tf.square(x - center_x) + tf.math.square(y - center_y)) / (tf.math.square(sigma) * 2)) * scale, dtype=tf.float32)\n",
    "\n",
    "        # part of the patch could be out of the boundary, so we need to determine the valid range\n",
    "        # if xmin = -2, it means the 2 left-most columns are invalid, which is max(0, -(-2)) = 2\n",
    "        patch_xmin = tf.math.maximum(0, -xmin)\n",
    "        patch_ymin = tf.math.maximum(0, -ymin)\n",
    "        # if xmin = 59, xmax = 66, but our output is 64x64, then we should discard 2 right-most columns\n",
    "        # which is min(64, 66) - 59 = 5, and column 6 and 7 are discarded\n",
    "        patch_xmax = tf.math.minimum(xmax, width) - xmin\n",
    "        patch_ymax = tf.math.minimum(ymax, height) - ymin\n",
    "\n",
    "        # also, we need to determine where to put this patch in the whole heatmap\n",
    "        heatmap_xmin = tf.math.maximum(0, xmin)\n",
    "        heatmap_ymin = tf.math.maximum(0, ymin)\n",
    "        heatmap_xmax = tf.math.minimum(xmax, width)\n",
    "        heatmap_ymax = tf.math.minimum(ymax, height)\n",
    "\n",
    "        # finally, insert this patch into the heatmap\n",
    "        indices = tf.TensorArray(tf.int32, 1, dynamic_size=True)\n",
    "        updates = tf.TensorArray(tf.float32, 1, dynamic_size=True)\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in tf.range(patch_ymin, patch_ymax):\n",
    "            for i in tf.range(patch_xmin, patch_xmax):\n",
    "                indices = indices.write(count, [heatmap_ymin+j, heatmap_xmin+i])\n",
    "                updates = updates.write(count, gaussian_patch[j][i])\n",
    "                count += 1\n",
    "\n",
    "        heatmap = tf.tensor_scatter_nd_update(heatmap, indices.stack(), updates.stack())\n",
    "\n",
    "        return heatmap\n",
    "\n",
    "\n",
    "    def make_heatmaps(self, features, keypoint_x, keypoint_y):\n",
    "        v = tf.cast(tf.sparse.to_dense(features['image/object/parts/v']), dtype=tf.float32)\n",
    "        x = tf.cast(tf.math.round(keypoint_x * self.heatmap_shape[0]), dtype=tf.int32)\n",
    "        y = tf.cast(tf.math.round(keypoint_y * self.heatmap_shape[1]), dtype=tf.int32)\n",
    "\n",
    "        num_heatmap = self.heatmap_shape[2]\n",
    "        heatmap_array = tf.TensorArray(tf.float32, 16)\n",
    "\n",
    "        for i in range(num_heatmap):\n",
    "            gaussian = self.generate_2d_guassian(self.heatmap_shape[1], self.heatmap_shape[0], y[i], x[i], v[i])\n",
    "            heatmap_array = heatmap_array.write(i, gaussian)\n",
    "\n",
    "        heatmaps = heatmap_array.stack()\n",
    "        heatmaps = tf.transpose(heatmaps, perm=[1, 2, 0]) # change to (64, 64, 16)\n",
    "\n",
    "        return heatmaps\n",
    "\n",
    "    def parse_tfexample(self, example_proto):\n",
    "        image_feature_description = {\n",
    "            'image/height': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/width': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/depth': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/parts/x': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/y': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/parts/v': tf.io.VarLenFeature(tf.int64),\n",
    "            'image/object/center/x': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/center/y': tf.io.FixedLenFeature([], tf.int64),\n",
    "            'image/object/scale': tf.io.FixedLenFeature([], tf.float32),\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/filename': tf.io.FixedLenFeature([], tf.string),\n",
    "        }\n",
    "        return tf.io.parse_single_example(example_proto,\n",
    "                                          image_feature_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourlass 모델 만들기\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Add, Concatenate, Lambda\n",
    "from tensorflow.keras.layers import Input, Conv2D, ReLU, MaxPool2D\n",
    "from tensorflow.keras.layers import UpSampling2D, ZeroPadding2D\n",
    "from tensorflow.keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BottleneckBlock(inputs, filters, strides=1, downsample=False, name=None):\n",
    "    identity = inputs\n",
    "    if downsample:\n",
    "        identity = Conv2D(\n",
    "            filters=filters,  # lift channels first\n",
    "            kernel_size=1,\n",
    "            strides=strides,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(inputs)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(inputs)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters // 2,\n",
    "        kernel_size=3,\n",
    "        strides=strides,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(x)\n",
    "\n",
    "    x = Add()([identity, x])\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HourglassModule(inputs, order, filters, num_residual):\n",
    "    \"\"\"\n",
    "    https://github.com/princeton-vl/pose-hg-train/blob/master/src/models/hg.lua#L3\n",
    "    \"\"\"\n",
    "    # Upper branch\n",
    "    up1 = BottleneckBlock(inputs, filters, downsample=False)\n",
    "\n",
    "    for i in range(num_residual):\n",
    "        up1 = BottleneckBlock(up1, filters, downsample=False)\n",
    "\n",
    "    # Lower branch\n",
    "    low1 = MaxPool2D(pool_size=2, strides=2)(inputs)\n",
    "    for i in range(num_residual):\n",
    "        low1 = BottleneckBlock(low1, filters, downsample=False)\n",
    "\n",
    "    low2 = low1\n",
    "    if order > 1:\n",
    "        low2 = HourglassModule(low1, order - 1, filters, num_residual)\n",
    "    else:\n",
    "        for i in range(num_residual):\n",
    "            low2 = BottleneckBlock(low2, filters, downsample=False)\n",
    "\n",
    "    low3 = low2\n",
    "    for i in range(num_residual):\n",
    "        low3 = BottleneckBlock(low3, filters, downsample=False)\n",
    "\n",
    "    up2 = UpSampling2D(size=2)(low3)\n",
    "\n",
    "    return up2 + up1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LinearLayer(inputs, filters):\n",
    "    x = Conv2D(\n",
    "        filters=filters,\n",
    "        kernel_size=1,\n",
    "        strides=1,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StackedHourglassNetwork(\n",
    "        input_shape=(256, 256, 3), num_stack=4, num_residual=1,\n",
    "        num_heatmap=16):\n",
    "    \"\"\"\n",
    "    https://github.com/princeton-vl/pose-hg-train/blob/master/src/models/hg.lua#L33\n",
    "    \"\"\"\n",
    "    inputs = Input(shape=input_shape)\n",
    "\n",
    "    # initial processing of the image\n",
    "    x = Conv2D(\n",
    "        filters=64,\n",
    "        kernel_size=7,\n",
    "        strides=2,\n",
    "        padding='same',\n",
    "        kernel_initializer='he_normal')(inputs)\n",
    "    x = BatchNormalization(momentum=0.9)(x)\n",
    "    x = ReLU()(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=True)\n",
    "    x = MaxPool2D(pool_size=2, strides=2)(x)\n",
    "    x = BottleneckBlock(x, 128, downsample=False)\n",
    "    x = BottleneckBlock(x, 256, downsample=True)\n",
    "\n",
    "    ys = []\n",
    "    for i in range(num_stack):\n",
    "        x = HourglassModule(x, order=4, filters=256, num_residual=num_residual)\n",
    "        for i in range(num_residual):\n",
    "            x = BottleneckBlock(x, 256, downsample=False)\n",
    "\n",
    "        # predict 256 channels like a fully connected layer.\n",
    "        x = LinearLayer(x, 256)\n",
    "\n",
    "        # predict final channels, which is also the number of predicted heatmap\n",
    "        y = Conv2D(\n",
    "            filters=num_heatmap,\n",
    "            kernel_size=1,\n",
    "            strides=1,\n",
    "            padding='same',\n",
    "            kernel_initializer='he_normal')(x)\n",
    "        ys.append(y)\n",
    "\n",
    "        # if it's not the last stack, we need to add predictions back\n",
    "        if i < num_stack - 1:\n",
    "            y_intermediate_1 = Conv2D(filters=256, kernel_size=1, strides=1)(x)\n",
    "            y_intermediate_2 = Conv2D(filters=256, kernel_size=1, strides=1)(y)\n",
    "            x = Add()([y_intermediate_1, y_intermediate_2])\n",
    "\n",
    "    return tf.keras.Model(inputs, ys, name='stacked_hourglass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 엔진 만들기\n",
    "\n",
    "import math\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "from datetime import datetime\n",
    "\n",
    "import click\n",
    "import tensorflow as tf\n",
    "\n",
    "from hourglass104 import StackedHourglassNetwork\n",
    "from preprocess import Preprocessor\n",
    "\n",
    "IMAGE_SHAPE = (256, 256, 3)\n",
    "HEATMAP_SIZE = (64, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def automatic_gpu_usage() :\n",
    "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        try:\n",
    "            # Currently, memory growth needs to be the same across GPUs\n",
    "            for gpu in gpus:\n",
    "                tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "        except RuntimeError as e:\n",
    "            # Memory growth must be set before GPUs have been initialized\n",
    "            print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer(object):\n",
    "    def __init__(self,\n",
    "                 model,\n",
    "                 epochs,\n",
    "                 global_batch_size,\n",
    "                 strategy,\n",
    "                 initial_learning_rate,\n",
    "                 version='0.0.1',\n",
    "                 start_epoch=1,\n",
    "                 tensorboard_dir='./logs'):\n",
    "        self.start_epoch = start_epoch\n",
    "        self.model = model\n",
    "        self.epochs = epochs\n",
    "        self.strategy = strategy\n",
    "        self.global_batch_size = global_batch_size\n",
    "        self.loss_object = tf.keras.losses.MeanSquaredError(\n",
    "            reduction=tf.keras.losses.Reduction.NONE)\n",
    "        # \"we use rmsprop with a learning rate of 2.5e-4.\"\"\n",
    "        self.optimizer = tf.keras.optimizers.Adam(\n",
    "            learning_rate=initial_learning_rate)\n",
    "        self.model = model\n",
    "\n",
    "        self.current_learning_rate = initial_learning_rate\n",
    "        self.last_val_loss = math.inf\n",
    "        self.lowest_val_loss = math.inf\n",
    "        self.patience_count = 0\n",
    "        self.max_patience = 10\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "        self.best_model = None\n",
    "        self.version = version\n",
    "\n",
    "    def lr_decay(self):\n",
    "        if self.patience_count >= self.max_patience:\n",
    "            self.current_learning_rate /= 10.0\n",
    "            self.patience_count = 0\n",
    "        elif self.last_val_loss == self.lowest_val_loss:\n",
    "            self.patience_count = 0\n",
    "        self.patience_count += 1\n",
    "\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def lr_decay_step(self, epoch):\n",
    "        if epoch == 25 or epoch == 50 or epoch == 75:\n",
    "            self.current_learning_rate /= 10.0\n",
    "        self.optimizer.learning_rate = self.current_learning_rate\n",
    "\n",
    "    def compute_loss(self, labels, outputs):\n",
    "        loss = 0\n",
    "        for output in outputs:\n",
    "            weights = tf.cast(labels > 0, dtype=tf.float32) * 81 + 1\n",
    "            loss += tf.math.reduce_mean(\n",
    "                tf.math.square(labels - output) * weights) * (\n",
    "                    1. / self.global_batch_size)\n",
    "        return loss\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        with tf.GradientTape() as tape:\n",
    "            outputs = self.model(images, training=True)\n",
    "            loss = self.compute_loss(labels, outputs)\n",
    "\n",
    "        grads = tape.gradient(\n",
    "            target=loss, sources=self.model.trainable_variables)\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(grads, self.model.trainable_variables))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def val_step(self, inputs):\n",
    "        images, labels = inputs\n",
    "        outputs = self.model(images, training=False)\n",
    "        loss = self.compute_loss(labels, outputs)\n",
    "        return loss\n",
    "\n",
    "    def run(self, train_dist_dataset, val_dist_dataset):\n",
    "        @tf.function\n",
    "        def distributed_train_epoch(dataset):\n",
    "            tf.print('Start distributed traininng...')\n",
    "            total_loss = 0.0\n",
    "            num_train_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                    self.train_step, args=(one_batch, ))\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                total_loss += batch_loss\n",
    "                num_train_batches += 1\n",
    "                tf.print('Trained batch', num_train_batches, 'batch loss',\n",
    "                         batch_loss, 'epoch total loss', total_loss / num_train_batches)\n",
    "            return total_loss, num_train_batches\n",
    "\n",
    "        @tf.function\n",
    "        def distributed_val_epoch(dataset):\n",
    "            total_loss = 0.0\n",
    "            num_val_batches = 0.0\n",
    "            for one_batch in dataset:\n",
    "                per_replica_loss = self.strategy.experimental_run_v2(\n",
    "                    self.val_step, args=(one_batch, ))\n",
    "                num_val_batches += 1\n",
    "                batch_loss = self.strategy.reduce(\n",
    "                    tf.distribute.ReduceOp.SUM, per_replica_loss, axis=None)\n",
    "                tf.print('Validated batch', num_val_batches, 'batch loss',\n",
    "                         batch_loss)\n",
    "                if not tf.math.is_nan(batch_loss):\n",
    "                    # TODO: Find out why the last validation batch loss become NaN\n",
    "                    total_loss += batch_loss\n",
    "                else:\n",
    "                    num_val_batches -= 1\n",
    "\n",
    "            return total_loss, num_val_batches\n",
    "\n",
    "        summary_writer = tf.summary.create_file_writer(self.tensorboard_dir)\n",
    "        summary_writer.set_as_default()\n",
    "\n",
    "        for epoch in range(self.start_epoch, self.epochs + 1):\n",
    "            tf.summary.experimental.set_step(epoch)\n",
    "\n",
    "            self.lr_decay()\n",
    "            tf.summary.scalar('epoch learning rate',\n",
    "                              self.current_learning_rate)\n",
    "\n",
    "            print('Start epoch {} with learning rate {}'.format(\n",
    "                epoch, self.current_learning_rate))\n",
    "\n",
    "            train_total_loss, num_train_batches = distributed_train_epoch(\n",
    "                train_dist_dataset)\n",
    "            train_loss = train_total_loss / num_train_batches\n",
    "            print('Epoch {} train loss {}'.format(epoch, train_loss))\n",
    "            tf.summary.scalar('epoch train loss', train_loss)\n",
    "\n",
    "            val_total_loss, num_val_batches = distributed_val_epoch(\n",
    "                val_dist_dataset)\n",
    "            val_loss = val_total_loss / num_val_batches\n",
    "            print('Epoch {} val loss {}'.format(epoch, val_loss))\n",
    "            tf.summary.scalar('epoch val loss', val_loss)\n",
    "\n",
    "            # save model when reach a new lowest validation loss\n",
    "            if val_loss < self.lowest_val_loss:\n",
    "                self.save_model(epoch, val_loss)\n",
    "                self.lowest_val_loss = val_loss\n",
    "            self.last_val_loss = val_loss\n",
    "\n",
    "        return self.best_model\n",
    "\n",
    "    def save_model(self, epoch, loss):\n",
    "        model_name = './models/model-v{}-epoch-{}-loss-{:.4f}.h5'.format(\n",
    "            self.version, epoch, loss)\n",
    "        self.model.save_weights(model_name)\n",
    "        self.best_model = model_name\n",
    "        print(\"Model {} saved.\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(tfrecords, batch_size, num_heatmap, is_train):\n",
    "    preprocess = Preprocessor(\n",
    "        IMAGE_SHAPE, (HEATMAP_SIZE[0], HEATMAP_SIZE[1], num_heatmap), is_train)\n",
    "\n",
    "    dataset = tf.data.Dataset.list_files(tfrecords)\n",
    "    dataset = tf.data.TFRecordDataset(dataset)\n",
    "    dataset = dataset.map(\n",
    "        preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    if is_train:\n",
    "        dataset = dataset.shuffle(batch_size)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, start_epoch, learning_rate, tensorboard_dir, checkpoint,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords, version):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    global_batch_size = strategy.num_replicas_in_sync * batch_size\n",
    "    train_dataset = create_dataset(\n",
    "        train_tfrecords, global_batch_size, num_heatmap, is_train=True)\n",
    "    val_dataset = create_dataset(\n",
    "        val_tfrecords, global_batch_size, num_heatmap, is_train=False)\n",
    "\n",
    "    if not os.path.exists(os.path.join('./models')):\n",
    "        os.makedirs(os.path.join('./models/'))\n",
    "\n",
    "    with strategy.scope():\n",
    "        train_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            train_dataset)\n",
    "        val_dist_dataset = strategy.experimental_distribute_dataset(\n",
    "            val_dataset)\n",
    "\n",
    "        model = StackedHourglassNetwork(IMAGE_SHAPE, 4, 1, num_heatmap)\n",
    "        if checkpoint and os.path.exists(checkpoint):\n",
    "            model.load_weights(checkpoint)\n",
    "\n",
    "        trainer = Trainer(\n",
    "            model,\n",
    "            epochs,\n",
    "            global_batch_size,\n",
    "            strategy,\n",
    "            initial_learning_rate=learning_rate,\n",
    "            start_epoch=start_epoch,\n",
    "            version=version,\n",
    "            tensorboard_dir=tensorboard_dir)\n",
    "\n",
    "        print('Start training...')\n",
    "        return trainer.run(train_dist_dataset, val_dist_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n",
      "Start training...\n",
      "Start epoch 1 with learning rate 0.0007\n",
      "Start distributed traininng...\n",
      "Trained batch 1 batch loss 2.54774523 epoch total loss 2.54774523\n",
      "Trained batch 2 batch loss 2.51231098 epoch total loss 2.5300281\n",
      "Trained batch 3 batch loss 2.48931503 epoch total loss 2.51645708\n",
      "Trained batch 4 batch loss 2.50051141 epoch total loss 2.51247072\n",
      "Trained batch 5 batch loss 2.36756444 epoch total loss 2.48348951\n",
      "Trained batch 6 batch loss 2.33271837 epoch total loss 2.45836091\n",
      "Trained batch 7 batch loss 2.30996966 epoch total loss 2.43716216\n",
      "Trained batch 8 batch loss 2.3161428 epoch total loss 2.42203474\n",
      "Trained batch 9 batch loss 2.35302877 epoch total loss 2.41436744\n",
      "Trained batch 10 batch loss 2.30798912 epoch total loss 2.40372968\n",
      "Trained batch 11 batch loss 2.22384167 epoch total loss 2.38737631\n",
      "Trained batch 12 batch loss 1.99653029 epoch total loss 2.35480571\n",
      "Trained batch 13 batch loss 1.88438296 epoch total loss 2.31861925\n",
      "Trained batch 14 batch loss 1.83865428 epoch total loss 2.28433609\n",
      "Trained batch 15 batch loss 1.83786273 epoch total loss 2.2545712\n",
      "Trained batch 16 batch loss 1.91558373 epoch total loss 2.23338461\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[16,256,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node while/body/_10/gradient_tape/while/stacked_hourglass/conv2d_241/Conv2D/Conv2DBackpropInput}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[while/LoopCond/_3323/_524]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[16,256,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node while/body/_10/gradient_tape/while/stacked_hourglass/conv2d_241/Conv2D/Conv2DBackpropInput}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_train_epoch_151460]\n\nFunction call stack:\ndistributed_train_epoch -> distributed_train_epoch\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-640d09823df8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     train(epochs, start_epoch, learning_rate, tensorboard_dir, pretrained_path,\n\u001b[0;32m---> 17\u001b[0;31m           num_heatmap, batch_size, train_tfrecords, val_tfrecords, '0.0.1')\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-19bfdd01aecc>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, start_epoch, learning_rate, tensorboard_dir, checkpoint, num_heatmap, batch_size, train_tfrecords, val_tfrecords, version)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Start training...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dist_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dist_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-03c195e2985a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, train_dist_dataset, val_dist_dataset)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             train_total_loss, num_train_batches = distributed_train_epoch(\n\u001b[0;32m--> 126\u001b[0;31m                 train_dist_dataset)\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_total_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_train_batches\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch {} train loss {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    838\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 840\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1846\u001b[0m                            resource_variable_ops.BaseResourceVariable))],\n\u001b[1;32m   1847\u001b[0m         \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m         cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1923\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1924\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1926\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    548\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/aiffel/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: 2 root error(s) found.\n  (0) Resource exhausted:  OOM when allocating tensor with shape[16,256,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node while/body/_10/gradient_tape/while/stacked_hourglass/conv2d_241/Conv2D/Conv2DBackpropInput}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[while/LoopCond/_3323/_524]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n  (1) Resource exhausted:  OOM when allocating tensor with shape[16,256,64,64] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node while/body/_10/gradient_tape/while/stacked_hourglass/conv2d_241/Conv2D/Conv2DBackpropInput}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n0 successful operations.\n0 derived errors ignored. [Op:__inference_distributed_train_epoch_151460]\n\nFunction call stack:\ndistributed_train_epoch -> distributed_train_epoch\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    tfrecords_dir = './dataset/tfrecords_mpii/'\n",
    "    train_tfrecords = os.path.join(tfrecords_dir, 'train*')\n",
    "    val_tfrecords = os.path.join(tfrecords_dir, 'val*')\n",
    "    epochs = 50\n",
    "    batch_size = 16\n",
    "    num_heatmap = 16\n",
    "    tensorboard_dir = './logs/'\n",
    "    learning_rate = 0.0007\n",
    "    start_epoch = 1\n",
    "\n",
    "    automatic_gpu_usage()\n",
    "\n",
    "    pretrained_path = None # './models_old/model-v0.0.2-epoch-15-loss-1.1013.h5'\n",
    "\n",
    "    train(epochs, start_epoch, learning_rate, tensorboard_dir, pretrained_path,\n",
    "          num_heatmap, batch_size, train_tfrecords, val_tfrecords, '0.0.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. simplebaseline 모델로 변경하여 훈련하기\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 두 모델의 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
