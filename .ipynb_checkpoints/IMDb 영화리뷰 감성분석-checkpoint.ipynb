{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 임포트\n",
    "\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋 다운로드\n",
    "\n",
    "imdb = keras.datasets.imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 샘플 개수 : 25000, 테스트 개수 :25000\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분리\n",
    "\n",
    "(x_train, y_train),(x_test, y_test) = imdb.load_data(num_words = 10000)\n",
    "print('훈련 샘플 개수 : {}, 테스트 개수 :{}'.format(len(x_train), len(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
      "라벨 :  1\n",
      "1번째 리뷰 문장 길이 :  218\n",
      "2번째 리뷰 문장 길이 :  189\n"
     ]
    }
   ],
   "source": [
    "# 데이터 확인 (encode 된 데이터를 다운 받음을 알 수 있다.)\n",
    "\n",
    "print(x_train[0])\n",
    "print('라벨 : ', y_train[0])\n",
    "print('1번째 리뷰 문장 길이 : ', len(x_train[0]))\n",
    "print('2번째 리뷰 문장 길이 : ', len(x_train[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# IMDb 데이터셋은 encode 에 사용한 딕셔너리도 제공한다.\n",
    "\n",
    "word_to_index = imdb.get_word_index()\n",
    "index_to_word = {index : word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])\n",
    "print(word_to_index['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOS>\n",
      "4\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "# 텍스트 인코딩을 위한 word_to_index, index_to_word 보정\n",
    "\n",
    "# 실제 인코딩 인덱스는 제공된 word_to_index 기준 3씩 밀려있다.\n",
    "word_to_index = {k :(v + 3) for k, v in word_to_index.items()}\n",
    "\n",
    "word_to_index['<PAD>'] = 0\n",
    "word_to_index['<BOS>'] = 1\n",
    "word_to_index['<UNK>'] = 2\n",
    "word_to_index['<UNUSED>'] = 3\n",
    "\n",
    "index_to_word[0] = '<PAD>'\n",
    "index_to_word[1] = '<BOS>'\n",
    "index_to_word[2] = '<UNK>'\n",
    "index_to_word[3] = '<UNUSED>'\n",
    "\n",
    "index_to_word = {index : word for word, index in word_to_index.items()}\n",
    "\n",
    "print(index_to_word[1])\n",
    "print(word_to_index['the'])\n",
    "print(index_to_word[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 13, 1900, 6803]\n"
     ]
    }
   ],
   "source": [
    "# 문장 1개를 활용할 딕셔너리와 함께 주면, 단어 인덱스 리스트로 변환해 주는 함수\n",
    "# 단, 모든 문장은 <BOS>로 시작 \n",
    "def get_encoded_sentence(sentence, word_to_index) : \n",
    "    return [word_to_index['<BOS>']] + [word_to_index[word] if word in word_to_index else word_to_index['<UNK>'] for word in sentence.split()]\n",
    "print(get_encoded_sentence('i eat lunch', word_to_index))\n",
    "\n",
    "# 여러 개의 문장 리스트를 한꺼번에 숫자 텐서로 encode 해주는 함수\n",
    "def get_encoded_sentences(sentences, word_to_index) : \n",
    "    return [get_encoded_sentence(sentence, word_to_index) for sentence in sentences]\n",
    "\n",
    "# 숫자 벡터로 encode 된 문장을 원래대로 decode 하는 함수\n",
    "# [1 : ] 를 통해 <BOS> 를 제외\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word) :\n",
    "    return ''.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1 : ])\n",
    "\n",
    "# 여러개의 숫자 벡터로 encode 된 문장을 한꺼번에 원래대로 decoded 하는 함수\n",
    "def get_decoded_sentences(encoded_sentences, index_to_word) :\n",
    "    return [get_decoded_sentence(encoded_sentence, index_to_word) for encoded_sentence in encoded_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thisfilmwasjustbrilliantcastinglocationscenerystorydirectioneveryone'sreallysuitedtheparttheyplayedandyoucouldjustimaginebeingthererobert<UNK>isanamazingactorandnowthesamebeingdirector<UNK>fathercamefromthesamescottishislandasmyselfsoilovedthefacttherewasarealconnectionwiththisfilmthewittyremarksthroughoutthefilmweregreatitwasjustbrilliantsomuchthatiboughtthefilmassoonasitwasreleasedfor<UNK>andwouldrecommendittoeveryonetowatchandtheflyfishingwasamazingreallycriedattheenditwassosadandyouknowwhattheysayifyoucryatafilmitmusthavebeengoodandthisdefinitelywasalso<UNK>tothetwolittleboy'sthatplayedthe<UNK>ofnormanandpaultheywerejustbrilliantchildrenareoftenleftoutofthe<UNK>listithinkbecausethestarsthatplaythemallgrownuparesuchabigprofileforthewholefilmbutthesechildrenareamazingandshouldbepraisedforwhattheyhavedonedon'tyouthinkthewholestorywassolovelybecauseitwastrueandwassomeone'slifeafterallthatwassharedwithusall\n",
      "라벨 :  1\n"
     ]
    }
   ],
   "source": [
    "# encode 된 텍스트가 정상적으로 decode 되는지 확인\n",
    "\n",
    "# 숫자 벡터로 encode 된 문장을 원래대로 decode 하는 함수\n",
    "# [1 : ] 를 통해 <BOS> 를 제외\n",
    "def get_decoded_sentence(encoded_sentence, index_to_word) :\n",
    "    return ''.join(index_to_word[index] if index in index_to_word else '<UNK>' for index in encoded_sentence[1 : ])\n",
    "\n",
    "print(get_decoded_sentence(x_train[0], index_to_word))\n",
    "print('라벨 : ', y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장길이 평균 :  234.75892\n",
      "문장길이 최대값 :  234.75892\n",
      "문장길이 표준편차 :  234.75892\n",
      "pad sequences maxlen :  580\n",
      "전체 문장의 0.94536% 가 maxlen 설정값 이내에 포함됩니다.\n"
     ]
    }
   ],
   "source": [
    "# 문장의 최대 길이인 maxlen 값은 모델 성능에 영향을 미치기에 \n",
    "# 적절한 값을 찾기 위해 데이터셋의 분포를 확인해야 한다.\n",
    "\n",
    "total_data_text = list(x_train) + list(x_test)\n",
    "\n",
    "# 텍스트 데이터 문장길이의 리스트 생성\n",
    "num_tokens = [len(tokens) for tokens in total_data_text]\n",
    "num_tokens = np.array(num_tokens)\n",
    "\n",
    "# 문장길이의 평균값 최대값, 표준편차를 계산\n",
    "print(\"문장길이 평균 : \", np.mean(num_tokens))\n",
    "print(\"문장길이 최대값 : \", np.mean(num_tokens))\n",
    "print(\"문장길이 표준편차 : \", np.mean(num_tokens))\n",
    "\n",
    "# 최대 길이를 평균 + 2 * 표준편차로 수정\n",
    "max_tokens = np.mean(num_tokens) + (2 * np.std(num_tokens))\n",
    "maxlen = int(max_tokens)\n",
    "\n",
    "print('pad sequences maxlen : ', maxlen)\n",
    "print('전체 문장의 {}% 가 maxlen 설정값 이내에 포함됩니다.'.format(np.sum(num_tokens < max_tokens) / len(num_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 580)\n"
     ]
    }
   ],
   "source": [
    "# padding 을 post 에 하는 것과 pre 에 함에 따라 성능 차이가 발생\n",
    "# 따라서 각각 학습시켜보며 결과를 비교하여 선택\n",
    "# 일반적으로 마지막 입력이 최종 state 값에 영향을 미치므로,\n",
    "# 마지막 입력이 무의미한 padding 으로 채워지는 것은 비효율 적이다.\n",
    "\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, value = word_to_index['<PAD>'], padding = 'pre', maxlen = maxlen)\n",
    "\n",
    "x_test = keras.preprocessing.sequence.pad_sequences(x_test, value = word_to_index['<PAD>'], padding = 'pre', maxlen = maxlen)\n",
    "\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 16)          160000    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 8)                 800       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 160,881\n",
      "Trainable params: 160,881\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN 모델 설계\n",
    "\n",
    "vocab_size = 10000    # 어휘사전의 크기\n",
    "word_vector_dim = 16    # 워드 벡터의 차원수 (변경 가능한 파라미터)\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(vocab_size, word_vector_dim, input_shape = (None, )))\n",
    "model.add(keras.layers.LSTM(8))\n",
    "model.add(keras.layers.Dense(8, activation = 'relu'))\n",
    "model.add(keras.layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 580)\n",
      "(15000,)\n"
     ]
    }
   ],
   "source": [
    "# 훈련용 데이터에서 10,000 건을 분리하여 validation 셋으로 사용\n",
    "\n",
    "x_val = x_train[ : 10000]\n",
    "y_val = y_train[ : 10000]\n",
    "\n",
    "partial_x_train = x_train[10000 : ]\n",
    "partial_y_train = y_train[10000 : ]\n",
    "\n",
    "print(partial_x_train.shape)\n",
    "print(partial_y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "30/30 [==============================] - 2s 58ms/step - loss: 0.6010 - accuracy: 0.7181 - val_loss: 0.5971 - val_accuracy: 0.7191\n",
      "Epoch 2/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5832 - accuracy: 0.7281 - val_loss: 0.6068 - val_accuracy: 0.6990\n",
      "Epoch 3/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5726 - accuracy: 0.7339 - val_loss: 0.5859 - val_accuracy: 0.7221\n",
      "Epoch 4/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5635 - accuracy: 0.7411 - val_loss: 0.5819 - val_accuracy: 0.7217\n",
      "Epoch 5/50\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.5596 - accuracy: 0.7442 - val_loss: 0.5844 - val_accuracy: 0.7224\n",
      "Epoch 6/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5679 - accuracy: 0.7303 - val_loss: 0.5858 - val_accuracy: 0.7240\n",
      "Epoch 7/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5806 - accuracy: 0.7121 - val_loss: 0.5738 - val_accuracy: 0.7218\n",
      "Epoch 8/50\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.5552 - accuracy: 0.7658 - val_loss: 0.5796 - val_accuracy: 0.7457\n",
      "Epoch 9/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5647 - accuracy: 0.7489 - val_loss: 0.6074 - val_accuracy: 0.6928\n",
      "Epoch 10/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5597 - accuracy: 0.7425 - val_loss: 0.5699 - val_accuracy: 0.7297\n",
      "Epoch 11/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5436 - accuracy: 0.7578 - val_loss: 0.5597 - val_accuracy: 0.7532\n",
      "Epoch 12/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5312 - accuracy: 0.7734 - val_loss: 0.5566 - val_accuracy: 0.7541\n",
      "Epoch 13/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5758 - accuracy: 0.7440 - val_loss: 0.5679 - val_accuracy: 0.7360\n",
      "Epoch 14/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5827 - accuracy: 0.7331 - val_loss: 0.6008 - val_accuracy: 0.7367\n",
      "Epoch 15/50\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.5614 - accuracy: 0.7503 - val_loss: 0.5803 - val_accuracy: 0.7270\n",
      "Epoch 16/50\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.5523 - accuracy: 0.7533 - val_loss: 0.5712 - val_accuracy: 0.7383\n",
      "Epoch 17/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5489 - accuracy: 0.7577 - val_loss: 0.5709 - val_accuracy: 0.7348\n",
      "Epoch 18/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5496 - accuracy: 0.7540 - val_loss: 0.5696 - val_accuracy: 0.7363\n",
      "Epoch 19/50\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.5460 - accuracy: 0.7581 - val_loss: 0.5711 - val_accuracy: 0.7364\n",
      "Epoch 20/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5450 - accuracy: 0.7595 - val_loss: 0.5689 - val_accuracy: 0.7386\n",
      "Epoch 21/50\n",
      "30/30 [==============================] - 2s 52ms/step - loss: 0.5442 - accuracy: 0.7607 - val_loss: 0.5686 - val_accuracy: 0.7393\n",
      "Epoch 22/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5438 - accuracy: 0.7616 - val_loss: 0.5683 - val_accuracy: 0.7402\n",
      "Epoch 23/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5419 - accuracy: 0.7631 - val_loss: 0.5677 - val_accuracy: 0.7409\n",
      "Epoch 24/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5405 - accuracy: 0.7643 - val_loss: 0.5669 - val_accuracy: 0.7418\n",
      "Epoch 25/50\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.5397 - accuracy: 0.7649 - val_loss: 0.5665 - val_accuracy: 0.7421\n",
      "Epoch 26/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5392 - accuracy: 0.7655 - val_loss: 0.5654 - val_accuracy: 0.7433\n",
      "Epoch 27/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5392 - accuracy: 0.7656 - val_loss: 0.5646 - val_accuracy: 0.7445\n",
      "Epoch 28/50\n",
      "30/30 [==============================] - 2s 51ms/step - loss: 0.5378 - accuracy: 0.7669 - val_loss: 0.5646 - val_accuracy: 0.7444\n",
      "Epoch 29/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5376 - accuracy: 0.7674 - val_loss: 0.5658 - val_accuracy: 0.7440\n",
      "Epoch 30/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5359 - accuracy: 0.7687 - val_loss: 0.5642 - val_accuracy: 0.7453\n",
      "Epoch 31/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5348 - accuracy: 0.7695 - val_loss: 0.5640 - val_accuracy: 0.7458\n",
      "Epoch 32/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5352 - accuracy: 0.7691 - val_loss: 0.5631 - val_accuracy: 0.7463\n",
      "Epoch 33/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5353 - accuracy: 0.7689 - val_loss: 0.5628 - val_accuracy: 0.7464\n",
      "Epoch 34/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5350 - accuracy: 0.7697 - val_loss: 0.5617 - val_accuracy: 0.7481\n",
      "Epoch 35/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5338 - accuracy: 0.7703 - val_loss: 0.5614 - val_accuracy: 0.7476\n",
      "Epoch 36/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5339 - accuracy: 0.7696 - val_loss: 0.5608 - val_accuracy: 0.7480\n",
      "Epoch 37/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5324 - accuracy: 0.7709 - val_loss: 0.5600 - val_accuracy: 0.7485\n",
      "Epoch 38/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5316 - accuracy: 0.7715 - val_loss: 0.5585 - val_accuracy: 0.7497\n",
      "Epoch 39/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5338 - accuracy: 0.7690 - val_loss: 0.5586 - val_accuracy: 0.7493\n",
      "Epoch 40/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5339 - accuracy: 0.7688 - val_loss: 0.5567 - val_accuracy: 0.7511\n",
      "Epoch 41/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5321 - accuracy: 0.7701 - val_loss: 0.5564 - val_accuracy: 0.7510\n",
      "Epoch 42/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5317 - accuracy: 0.7700 - val_loss: 0.5567 - val_accuracy: 0.7506\n",
      "Epoch 43/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5351 - accuracy: 0.7651 - val_loss: 0.5638 - val_accuracy: 0.7384\n",
      "Epoch 44/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5362 - accuracy: 0.7633 - val_loss: 0.5574 - val_accuracy: 0.7515\n",
      "Epoch 45/50\n",
      "30/30 [==============================] - 1s 50ms/step - loss: 0.5481 - accuracy: 0.7527 - val_loss: 0.6591 - val_accuracy: 0.6381\n",
      "Epoch 46/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.6052 - accuracy: 0.6715 - val_loss: 0.5910 - val_accuracy: 0.6900\n",
      "Epoch 47/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5608 - accuracy: 0.7331 - val_loss: 0.5665 - val_accuracy: 0.7297\n",
      "Epoch 48/50\n",
      "30/30 [==============================] - 2s 50ms/step - loss: 0.5448 - accuracy: 0.7491 - val_loss: 0.5615 - val_accuracy: 0.7374\n",
      "Epoch 49/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5403 - accuracy: 0.7543 - val_loss: 0.5728 - val_accuracy: 0.7243\n",
      "Epoch 50/50\n",
      "30/30 [==============================] - 1s 49ms/step - loss: 0.5790 - accuracy: 0.7107 - val_loss: 0.7431 - val_accuracy: 0.5169\n"
     ]
    }
   ],
   "source": [
    "# 모델 학습\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "history = model.fit(partial_x_train, partial_y_train, epochs = epochs, batch_size = 512, validation_data = (x_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 8s - loss: 0.7432 - accuracy: 0.5177\n",
      "[0.7432294487953186, 0.5177199840545654]\n"
     ]
    }
   ],
   "source": [
    "# 테스트 셋으로 모델 평가\n",
    "\n",
    "results = model.evaluate(x_test, y_test, verbose = 2)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
